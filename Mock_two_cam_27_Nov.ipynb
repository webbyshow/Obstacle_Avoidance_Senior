{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Record Video and save\n",
    "care don't run the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "เริ่มบันทึกภาพอัตโนมัติทุกๆ 0.1 วินาที เป็นเวลา 20 วินาที...\n",
      "เซฟรูปภาพ: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Left5\\left_0000.jpg และ C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Right5\\right_0000.jpg\n",
      "เซฟรูปภาพ: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Left5\\left_0001.jpg และ C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Right5\\right_0001.jpg\n",
      "เซฟรูปภาพ: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Left5\\left_0002.jpg และ C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Right5\\right_0002.jpg\n",
      "เซฟรูปภาพ: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Left5\\left_0003.jpg และ C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Right5\\right_0003.jpg\n",
      "เซฟรูปภาพ: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Left5\\left_0004.jpg และ C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Right5\\right_0004.jpg\n",
      "เซฟรูปภาพ: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Left5\\left_0005.jpg และ C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Right5\\right_0005.jpg\n",
      "เซฟรูปภาพ: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Left5\\left_0006.jpg และ C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Right5\\right_0006.jpg\n",
      "เซฟรูปภาพ: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Left5\\left_0007.jpg และ C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Right5\\right_0007.jpg\n",
      "เซฟรูปภาพ: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Left5\\left_0008.jpg และ C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Right5\\right_0008.jpg\n",
      "เซฟรูปภาพ: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Left5\\left_0009.jpg และ C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Right5\\right_0009.jpg\n",
      "เซฟรูปภาพ: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Left5\\left_0010.jpg และ C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Right5\\right_0010.jpg\n",
      "เซฟรูปภาพ: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Left5\\left_0011.jpg และ C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Right5\\right_0011.jpg\n",
      "เซฟรูปภาพ: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Left5\\left_0012.jpg และ C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Right5\\right_0012.jpg\n",
      "เซฟรูปภาพ: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Left5\\left_0013.jpg และ C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Right5\\right_0013.jpg\n",
      "เซฟรูปภาพ: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Left5\\left_0014.jpg และ C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Right5\\right_0014.jpg\n",
      "เซฟรูปภาพ: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Left5\\left_0015.jpg และ C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Right5\\right_0015.jpg\n",
      "เซฟรูปภาพ: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Left5\\left_0016.jpg และ C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Right5\\right_0016.jpg\n",
      "เซฟรูปภาพ: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Left5\\left_0017.jpg และ C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Right5\\right_0017.jpg\n",
      "เซฟรูปภาพ: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Left5\\left_0018.jpg และ C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Right5\\right_0018.jpg\n",
      "เซฟรูปภาพ: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Left5\\left_0019.jpg และ C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Right5\\right_0019.jpg\n",
      "เซฟรูปภาพ: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Left5\\left_0020.jpg และ C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Right5\\right_0020.jpg\n",
      "เซฟรูปภาพ: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Left5\\left_0021.jpg และ C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Right5\\right_0021.jpg\n",
      "เซฟรูปภาพ: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Left5\\left_0022.jpg และ C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Right5\\right_0022.jpg\n",
      "เซฟรูปภาพ: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Left5\\left_0023.jpg และ C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Right5\\right_0023.jpg\n",
      "เซฟรูปภาพ: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Left5\\left_0024.jpg และ C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Right5\\right_0024.jpg\n",
      "เซฟรูปภาพ: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Left5\\left_0025.jpg และ C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Right5\\right_0025.jpg\n",
      "เซฟรูปภาพ: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Left5\\left_0026.jpg และ C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Right5\\right_0026.jpg\n",
      "เซฟรูปภาพ: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Left5\\left_0027.jpg และ C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Right5\\right_0027.jpg\n",
      "เซฟรูปภาพ: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Left5\\left_0028.jpg และ C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Right5\\right_0028.jpg\n",
      "เซฟรูปภาพ: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Left5\\left_0029.jpg และ C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Right5\\right_0029.jpg\n",
      "เซฟรูปภาพ: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Left5\\left_0030.jpg และ C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Right5\\right_0030.jpg\n",
      "เซฟรูปภาพ: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Left5\\left_0031.jpg และ C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Right5\\right_0031.jpg\n",
      "เซฟรูปภาพ: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Left5\\left_0032.jpg และ C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Right5\\right_0032.jpg\n",
      "เซฟรูปภาพ: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Left5\\left_0033.jpg และ C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Right5\\right_0033.jpg\n",
      "เซฟรูปภาพ: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Left5\\left_0034.jpg และ C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Right5\\right_0034.jpg\n",
      "เซฟรูปภาพ: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Left5\\left_0035.jpg และ C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Right5\\right_0035.jpg\n",
      "เซฟรูปภาพ: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Left5\\left_0036.jpg และ C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Right5\\right_0036.jpg\n",
      "เซฟรูปภาพ: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Left5\\left_0037.jpg และ C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Right5\\right_0037.jpg\n",
      "เซฟรูปภาพ: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Left5\\left_0038.jpg และ C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Right5\\right_0038.jpg\n",
      "เซฟรูปภาพ: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Left5\\left_0039.jpg และ C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Right5\\right_0039.jpg\n",
      "เซฟรูปภาพ: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Left5\\left_0040.jpg และ C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Right5\\right_0040.jpg\n",
      "เซฟรูปภาพ: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Left5\\left_0041.jpg และ C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Right5\\right_0041.jpg\n",
      "เซฟรูปภาพ: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Left5\\left_0042.jpg และ C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Right5\\right_0042.jpg\n",
      "เซฟรูปภาพ: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Left5\\left_0043.jpg และ C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Right5\\right_0043.jpg\n",
      "เซฟรูปภาพ: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Left5\\left_0044.jpg และ C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Right5\\right_0044.jpg\n",
      "เซฟรูปภาพ: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Left5\\left_0045.jpg และ C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Right5\\right_0045.jpg\n",
      "เซฟรูปภาพ: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Left5\\left_0046.jpg และ C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Right5\\right_0046.jpg\n",
      "เซฟรูปภาพ: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Left5\\left_0047.jpg และ C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Right5\\right_0047.jpg\n",
      "เซฟรูปภาพ: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Left5\\left_0048.jpg และ C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Right5\\right_0048.jpg\n",
      "เซฟรูปภาพ: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Left5\\left_0049.jpg และ C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Right5\\right_0049.jpg\n",
      "เซฟรูปภาพ: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Left5\\left_0050.jpg และ C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Right5\\right_0050.jpg\n",
      "เซฟรูปภาพ: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Left5\\left_0051.jpg และ C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Right5\\right_0051.jpg\n",
      "เซฟรูปภาพ: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Left5\\left_0052.jpg และ C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Right5\\right_0052.jpg\n",
      "เซฟรูปภาพ: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Left5\\left_0053.jpg และ C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Right5\\right_0053.jpg\n",
      "เซฟรูปภาพ: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Left5\\left_0054.jpg และ C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Right5\\right_0054.jpg\n",
      "เซฟรูปภาพ: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Left5\\left_0055.jpg และ C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Right5\\right_0055.jpg\n",
      "เซฟรูปภาพ: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Left5\\left_0056.jpg และ C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Right5\\right_0056.jpg\n",
      "เซฟรูปภาพ: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Left5\\left_0057.jpg และ C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Right5\\right_0057.jpg\n",
      "เซฟรูปภาพ: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Left5\\left_0058.jpg และ C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Right5\\right_0058.jpg\n",
      "เซฟรูปภาพ: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Left5\\left_0059.jpg และ C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Right5\\right_0059.jpg\n",
      "เซฟรูปภาพ: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Left5\\left_0060.jpg และ C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Right5\\right_0060.jpg\n",
      "เซฟรูปภาพ: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Left5\\left_0061.jpg และ C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Right5\\right_0061.jpg\n",
      "เซฟรูปภาพ: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Left5\\left_0062.jpg และ C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Right5\\right_0062.jpg\n",
      "เซฟรูปภาพ: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Left5\\left_0063.jpg และ C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Right5\\right_0063.jpg\n",
      "เซฟรูปภาพ: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Left5\\left_0064.jpg และ C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Right5\\right_0064.jpg\n",
      "เซฟรูปภาพ: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Left5\\left_0065.jpg และ C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Right5\\right_0065.jpg\n",
      "เซฟรูปภาพ: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Left5\\left_0066.jpg และ C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Right5\\right_0066.jpg\n",
      "เซฟรูปภาพ: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Left5\\left_0067.jpg และ C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Right5\\right_0067.jpg\n",
      "เซฟรูปภาพ: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Left5\\left_0068.jpg และ C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Right5\\right_0068.jpg\n",
      "เซฟรูปภาพ: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Left5\\left_0069.jpg และ C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Right5\\right_0069.jpg\n",
      "เซฟรูปภาพ: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Left5\\left_0070.jpg และ C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Right5\\right_0070.jpg\n",
      "เซฟรูปภาพ: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Left5\\left_0071.jpg และ C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Right5\\right_0071.jpg\n",
      "เซฟรูปภาพ: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Left5\\left_0072.jpg และ C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Right5\\right_0072.jpg\n",
      "เซฟรูปภาพ: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Left5\\left_0073.jpg และ C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Right5\\right_0073.jpg\n",
      "เซฟรูปภาพ: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Left5\\left_0074.jpg และ C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Right5\\right_0074.jpg\n",
      "เซฟรูปภาพ: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Left5\\left_0075.jpg และ C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Right5\\right_0075.jpg\n",
      "เซฟรูปภาพ: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Left5\\left_0076.jpg และ C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Right5\\right_0076.jpg\n",
      "เซฟรูปภาพ: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Left5\\left_0077.jpg และ C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Right5\\right_0077.jpg\n",
      "เซฟรูปภาพ: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Left5\\left_0078.jpg และ C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Right5\\right_0078.jpg\n",
      "เซฟรูปภาพ: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Left5\\left_0079.jpg และ C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Right5\\right_0079.jpg\n",
      "เซฟรูปภาพ: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Left5\\left_0080.jpg และ C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Right5\\right_0080.jpg\n",
      "เซฟรูปภาพ: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Left5\\left_0081.jpg และ C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Right5\\right_0081.jpg\n",
      "เซฟรูปภาพ: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Left5\\left_0082.jpg และ C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Right5\\right_0082.jpg\n",
      "เซฟรูปภาพ: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Left5\\left_0083.jpg และ C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Right5\\right_0083.jpg\n",
      "เซฟรูปภาพ: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Left5\\left_0084.jpg และ C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Right5\\right_0084.jpg\n",
      "เซฟรูปภาพ: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Left5\\left_0085.jpg และ C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Right5\\right_0085.jpg\n",
      "เซฟรูปภาพ: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Left5\\left_0086.jpg และ C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Right5\\right_0086.jpg\n",
      "เซฟรูปภาพ: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Left5\\left_0087.jpg และ C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Right5\\right_0087.jpg\n",
      "เซฟรูปภาพ: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Left5\\left_0088.jpg และ C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Right5\\right_0088.jpg\n",
      "เซฟรูปภาพ: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Left5\\left_0089.jpg และ C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Right5\\right_0089.jpg\n",
      "เซฟรูปภาพ: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Left5\\left_0090.jpg และ C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Right5\\right_0090.jpg\n",
      "เซฟรูปภาพ: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Left5\\left_0091.jpg และ C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Right5\\right_0091.jpg\n",
      "เซฟรูปภาพ: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Left5\\left_0092.jpg และ C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Right5\\right_0092.jpg\n",
      "เซฟรูปภาพ: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Left5\\left_0093.jpg และ C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Right5\\right_0093.jpg\n",
      "เซฟรูปภาพ: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Left5\\left_0094.jpg และ C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Right5\\right_0094.jpg\n",
      "เซฟรูปภาพ: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Left5\\left_0095.jpg และ C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Right5\\right_0095.jpg\n",
      "เซฟรูปภาพ: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Left5\\left_0096.jpg และ C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Right5\\right_0096.jpg\n",
      "เซฟรูปภาพ: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Left5\\left_0097.jpg และ C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Right5\\right_0097.jpg\n",
      "เซฟรูปภาพ: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Left5\\left_0098.jpg และ C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Right5\\right_0098.jpg\n",
      "เซฟรูปภาพ: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Left5\\left_0099.jpg และ C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Right5\\right_0099.jpg\n",
      "เซฟรูปภาพ: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Left5\\left_0100.jpg และ C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Right5\\right_0100.jpg\n",
      "สิ้นสุดการบันทึกภาพ...\n"
     ]
    }
   ],
   "source": [
    "jai yen\n",
    "import cv2\n",
    "import os\n",
    "import time\n",
    "\n",
    "# กำหนดโฟลเดอร์สำหรับเซฟรูปภาพ\n",
    "left_folder = r\"C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Left5\"\n",
    "right_folder = r\"C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Right5\"\n",
    "\n",
    "# สร้างโฟลเดอร์หากยังไม่มี\n",
    "os.makedirs(left_folder, exist_ok=True)\n",
    "os.makedirs(right_folder, exist_ok=True)\n",
    "\n",
    "# เปิดทั้งสองกล้อง (ใช้ Index 1 และ 2)\n",
    "cap_left = cv2.VideoCapture(1)  # กล้องซ้าย\n",
    "cap_right = cv2.VideoCapture(0)  # ตาซ้าย\n",
    "\n",
    "# ตรวจสอบว่าทั้งสองกล้องเปิดสำเร็จ\n",
    "if not cap_left.isOpened() or not cap_right.isOpened():\n",
    "    print(\"Error: ไม่สามารถเปิดกล้องได้\")\n",
    "    cap_left.release()\n",
    "    cap_right.release()\n",
    "    exit()\n",
    "\n",
    "# ตัวนับสำหรับชื่อไฟล์\n",
    "left_counter = 0\n",
    "right_counter = 0\n",
    "\n",
    "# ตั้งค่าเวลาเริ่มต้นและเวลาสิ้นสุด\n",
    "start_time = time.time()\n",
    "duration = 20  # ระยะเวลาบันทึกภาพ 10 วินาที\n",
    "interval = 0.1  # ความถี่ในการบันทึกภาพ (0.1 วินาที)\n",
    "\n",
    "print(f\"เริ่มบันทึกภาพอัตโนมัติทุกๆ {interval} วินาที เป็นเวลา {duration} วินาที...\")\n",
    "\n",
    "while True:\n",
    "    # อ่านเฟรมจากทั้งสองกล้อง\n",
    "    ret_left, frame_left = cap_left.read()\n",
    "    ret_right, frame_right = cap_right.read()\n",
    "\n",
    "    if not ret_left or not ret_right:\n",
    "        print(\"Error: อ่านเฟรมจากกล้องไม่สำเร็จ\")\n",
    "        break\n",
    "\n",
    "    # เวลาปัจจุบัน\n",
    "    current_time = time.time()\n",
    "\n",
    "    # ถ้าผ่านไปตาม interval ให้บันทึกภาพ\n",
    "    if current_time - start_time >= interval:\n",
    "        # สร้างชื่อไฟล์และเซฟรูปภาพ\n",
    "        left_filename = os.path.join(left_folder, f\"left_{left_counter:04d}.jpg\")\n",
    "        right_filename = os.path.join(right_folder, f\"right_{right_counter:04d}.jpg\")\n",
    "\n",
    "        cv2.imwrite(left_filename, frame_left)\n",
    "        cv2.imwrite(right_filename, frame_right) #save\n",
    "\n",
    "        print(f\"เซฟรูปภาพ: {left_filename} และ {right_filename}\")\n",
    "\n",
    "        # เพิ่มตัวนับ\n",
    "        left_counter += 1\n",
    "        right_counter += 1\n",
    "\n",
    "        # อัปเดตเวลาเริ่มต้นรอบใหม่\n",
    "        start_time = current_time\n",
    "\n",
    "    # แสดงภาพจากทั้งสองกล้อง\n",
    "    cv2.imshow(\"Left Camera\", frame_left)\n",
    "    cv2.imshow(\"Right Camera\", frame_right)\n",
    "\n",
    "    # ออกจากลูปหากเกินระยะเวลา 10 วินาที หรือกด 'q'\n",
    "    if current_time - start_time > duration or cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        print(\"สิ้นสุดการบันทึกภาพ...\")\n",
    "        break\n",
    "\n",
    "# ปล่อยทรัพยากร\n",
    "cap_left.release()\n",
    "cap_right.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Display Frames to Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying images. Press 'q' to quit.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Paths to the folders containing images\n",
    "left_folder = r\"C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Left5\"\n",
    "right_folder = r\"C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Right5\"\n",
    "\n",
    "# Get sorted list of image files\n",
    "left_images = sorted([os.path.join(left_folder, f) for f in os.listdir(left_folder) if f.endswith(('.png', '.jpg', '.jpeg'))])\n",
    "right_images = sorted([os.path.join(right_folder, f) for f in os.listdir(right_folder) if f.endswith(('.png', '.jpg', '.jpeg'))])\n",
    "\n",
    "# Check if both folders have the same number of images\n",
    "if len(left_images) != len(right_images):\n",
    "    print(\"Error: The number of images in the left and right folders does not match.\")\n",
    "    exit()\n",
    "\n",
    "print(\"Displaying images. Press 'q' to quit.\")\n",
    "\n",
    "# Display images sequentially\n",
    "for left_image_path, right_image_path in zip(left_images, right_images):\n",
    "    # Read images\n",
    "    left_image = cv2.imread(left_image_path)\n",
    "    right_image = cv2.imread(right_image_path)\n",
    "\n",
    "    if left_image is None or right_image is None:\n",
    "        print(f\"Error reading images: {left_image_path}, {right_image_path}\")\n",
    "        continue\n",
    "\n",
    "    # Resize images to the same size (if needed)\n",
    "    left_image = cv2.resize(left_image, (640, 480))\n",
    "    right_image = cv2.resize(right_image, (640, 480))\n",
    "\n",
    "    # Combine images side by side\n",
    "    combined_frame = np.hstack((left_image, right_image))\n",
    "\n",
    "    # Display the combined frame\n",
    "    cv2.imshow(\"Combined Camera Images\", combined_frame)\n",
    "\n",
    "    # Press 'q' to quit\n",
    "    if cv2.waitKey(100) & 0xFF == ord('q'):  # Display each frame for 100ms\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply YOLO v3 and save\n",
    "be careful! before run this cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5\\left_0000.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5\\right_0000.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5\\left_0001.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5\\right_0001.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5\\left_0002.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5\\right_0002.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5\\left_0003.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5\\right_0003.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5\\left_0004.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5\\right_0004.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5\\left_0005.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5\\right_0005.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5\\left_0006.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5\\right_0006.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5\\left_0007.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5\\right_0007.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5\\left_0008.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5\\right_0008.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5\\left_0009.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5\\right_0009.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5\\left_0010.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5\\right_0010.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5\\left_0011.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5\\right_0011.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5\\left_0012.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5\\right_0012.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5\\left_0013.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5\\right_0013.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5\\left_0014.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5\\right_0014.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5\\left_0015.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5\\right_0015.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5\\left_0016.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5\\right_0016.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5\\left_0017.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5\\right_0017.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5\\left_0018.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5\\right_0018.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5\\left_0019.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5\\right_0019.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5\\left_0020.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5\\right_0020.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5\\left_0021.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5\\right_0021.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5\\left_0022.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5\\right_0022.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5\\left_0023.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5\\right_0023.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5\\left_0024.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5\\right_0024.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5\\left_0025.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5\\right_0025.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5\\left_0026.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5\\right_0026.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5\\left_0027.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5\\right_0027.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5\\left_0028.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5\\right_0028.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5\\left_0029.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5\\right_0029.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5\\left_0030.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5\\right_0030.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5\\left_0031.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5\\right_0031.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5\\left_0032.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5\\right_0032.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5\\left_0033.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5\\right_0033.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5\\left_0034.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5\\right_0034.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5\\left_0035.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5\\right_0035.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5\\left_0036.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5\\right_0036.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5\\left_0037.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5\\right_0037.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5\\left_0038.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5\\right_0038.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5\\left_0039.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5\\right_0039.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5\\left_0040.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5\\right_0040.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5\\left_0041.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5\\right_0041.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5\\left_0042.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5\\right_0042.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5\\left_0043.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5\\right_0043.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5\\left_0044.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5\\right_0044.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5\\left_0045.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5\\right_0045.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5\\left_0046.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5\\right_0046.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5\\left_0047.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5\\right_0047.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5\\left_0048.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5\\right_0048.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5\\left_0049.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5\\right_0049.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5\\left_0050.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5\\right_0050.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5\\left_0051.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5\\right_0051.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5\\left_0052.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5\\right_0052.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5\\left_0053.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5\\right_0053.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5\\left_0054.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5\\right_0054.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5\\left_0055.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5\\right_0055.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5\\left_0056.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5\\right_0056.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5\\left_0057.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5\\right_0057.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5\\left_0058.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5\\right_0058.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5\\left_0059.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5\\right_0059.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5\\left_0060.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5\\right_0060.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5\\left_0061.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5\\right_0061.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5\\left_0062.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5\\right_0062.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5\\left_0063.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5\\right_0063.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5\\left_0064.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5\\right_0064.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5\\left_0065.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5\\right_0065.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5\\left_0066.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5\\right_0066.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5\\left_0067.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5\\right_0067.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5\\left_0068.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5\\right_0068.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5\\left_0069.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5\\right_0069.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5\\left_0070.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5\\right_0070.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5\\left_0071.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5\\right_0071.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5\\left_0072.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5\\right_0072.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5\\left_0073.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5\\right_0073.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5\\left_0074.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5\\right_0074.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5\\left_0075.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5\\right_0075.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5\\left_0076.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5\\right_0076.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5\\left_0077.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5\\right_0077.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5\\left_0078.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5\\right_0078.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5\\left_0079.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5\\right_0079.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5\\left_0080.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5\\right_0080.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5\\left_0081.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5\\right_0081.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5\\left_0082.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5\\right_0082.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5\\left_0083.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5\\right_0083.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5\\left_0084.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5\\right_0084.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5\\left_0085.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5\\right_0085.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5\\left_0086.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5\\right_0086.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5\\left_0087.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5\\right_0087.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5\\left_0088.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5\\right_0088.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5\\left_0089.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5\\right_0089.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5\\left_0090.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5\\right_0090.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5\\left_0091.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5\\right_0091.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5\\left_0092.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5\\right_0092.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5\\left_0093.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5\\right_0093.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5\\left_0094.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5\\right_0094.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5\\left_0095.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5\\right_0095.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5\\left_0096.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5\\right_0096.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5\\left_0097.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5\\right_0097.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5\\left_0098.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5\\right_0098.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5\\left_0099.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5\\right_0099.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5\\left_0100.jpg\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5\\right_0100.jpg\n",
      "Object detection completed for all images.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "# Paths to YOLOv3 files\n",
    "weights_path = r\"C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\yolov3\\yolov3.weights\"\n",
    "config_path = r\"C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\yolov3\\yolov3.cfg\"\n",
    "labels_path = r\"C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\yolov3\\coco.names\"\n",
    "\n",
    "\n",
    "# Load YOLO model\n",
    "net = cv2.dnn.readNetFromDarknet(config_path, weights_path)\n",
    "\n",
    "# Use GPU (optional)\n",
    "# net.setPreferableBackend(cv2.dnn.DNN_BACKEND_CUDA)\n",
    "# net.setPreferableTarget(cv2.dnn.DNN_TARGET_CUDA)\n",
    "\n",
    "# Load class labels\n",
    "with open(labels_path, \"r\") as f:\n",
    "    labels = f.read().strip().split(\"\\n\")\n",
    "\n",
    "# Set random colors for each label\n",
    "colors = [(255, 0, 0), (0, 255, 0), (0, 0, 255)] * len(labels)\n",
    "\n",
    "# Paths to the folders containing images\n",
    "left_folder = r\"C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Left5\"\n",
    "right_folder = r\"C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Right5\"\n",
    "\n",
    "# Output folders for processed images\n",
    "output_left_folder = r\"C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5\"\n",
    "output_right_folder = r\"C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5\"\n",
    "\n",
    "os.makedirs(output_left_folder, exist_ok=True)\n",
    "os.makedirs(output_right_folder, exist_ok=True)\n",
    "\n",
    "# Get sorted list of image files\n",
    "left_images = sorted([os.path.join(left_folder, f) for f in os.listdir(left_folder) if f.endswith(('.png', '.jpg', '.jpeg'))])\n",
    "right_images = sorted([os.path.join(right_folder, f) for f in os.listdir(right_folder) if f.endswith(('.png', '.jpg', '.jpeg'))])\n",
    "\n",
    "# Ensure both folders have the same number of images\n",
    "if len(left_images) != len(right_images):\n",
    "    print(\"Error: The number of images in the left and right folders does not match.\")\n",
    "    exit()\n",
    "\n",
    "# Process each pair of images\n",
    "for left_image_path, right_image_path in zip(left_images, right_images):\n",
    "    for image_path, output_folder in [(left_image_path, output_left_folder), (right_image_path, output_right_folder)]:\n",
    "        # Read the image\n",
    "        image = cv2.imread(image_path)\n",
    "        (H, W) = image.shape[:2]\n",
    "\n",
    "        # Prepare the image for YOLO\n",
    "        blob = cv2.dnn.blobFromImage(image, 1 / 255.0, (416, 416), swapRB=True, crop=False)\n",
    "        net.setInput(blob)\n",
    "        layer_names = net.getLayerNames()\n",
    "        output_layers = [layer_names[i - 1] for i in net.getUnconnectedOutLayers()]\n",
    "        layer_outputs = net.forward(output_layers)\n",
    "\n",
    "        # Initialize lists for bounding boxes, confidences, and class IDs\n",
    "        boxes = []\n",
    "        confidences = []\n",
    "        class_ids = []\n",
    "\n",
    "        for output in layer_outputs:\n",
    "            for detection in output:\n",
    "                scores = detection[5:]\n",
    "                class_id = int(scores.argmax())\n",
    "                confidence = scores[class_id]\n",
    "\n",
    "                if confidence > 0.5:  # Confidence threshold\n",
    "                    box = detection[0:4] * [W, H, W, H]\n",
    "                    (centerX, centerY, width, height) = box.astype(\"int\")\n",
    "                    x = int(centerX - (width / 2))\n",
    "                    y = int(centerY - (height / 2))\n",
    "\n",
    "                    boxes.append([x, y, int(width), int(height)])\n",
    "                    confidences.append(float(confidence))\n",
    "                    class_ids.append(class_id)\n",
    "\n",
    "        # Apply Non-Maxima Suppression\n",
    "        indices = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)\n",
    "\n",
    "        # Draw bounding boxes and labels on the image\n",
    "        if len(indices) > 0:\n",
    "            for i in indices.flatten():\n",
    "                (x, y, w, h) = boxes[i]\n",
    "                color = colors[class_ids[i] % len(colors)]\n",
    "                label = f\"{labels[class_ids[i]]}: {confidences[i]:.2f}\"\n",
    "                cv2.rectangle(image, (x, y), (x + w, y + h), color, 2)\n",
    "                cv2.putText(image, label, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "\n",
    "        # Save the processed image\n",
    "        output_path = os.path.join(output_folder, os.path.basename(image_path))\n",
    "        cv2.imwrite(output_path, image)\n",
    "        print(f\"Processed and saved: {output_path}\")\n",
    "\n",
    "print(\"Object detection completed for all images.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply YOLO v5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\webin\\anaconda3\\lib\\site-packages (2.5.1)\n",
      "Requirement already satisfied: torchvision in c:\\users\\webin\\anaconda3\\lib\\site-packages (0.20.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\webin\\anaconda3\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\webin\\anaconda3\\lib\\site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\webin\\anaconda3\\lib\\site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\webin\\anaconda3\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\webin\\anaconda3\\lib\\site-packages (from torch) (2024.3.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\webin\\anaconda3\\lib\\site-packages (from torch) (75.6.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\webin\\anaconda3\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\webin\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\webin\\anaconda3\\lib\\site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\webin\\anaconda3\\lib\\site-packages (from torchvision) (10.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\webin\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch torchvision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import time\n",
    "\n",
    "# Load the YOLOv5 model (use \"yolov5s\" for small and fast model)\n",
    "model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)\n",
    "\n",
    "# Paths to the folders containing images\n",
    "left_folder = r\"C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Left5\"\n",
    "right_folder = r\"C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Right5\"\n",
    "\n",
    "# Output folders for processed images\n",
    "output_left_folder = r\"C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5_YOLOv5\"\n",
    "output_right_folder = r\"C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5_YOLOv5\"\n",
    "\n",
    "os.makedirs(output_left_folder, exist_ok=True)\n",
    "os.makedirs(output_right_folder, exist_ok=True)\n",
    "\n",
    "# Get sorted list of image files\n",
    "left_images = sorted([os.path.join(left_folder, f) for f in os.listdir(left_folder) if f.endswith(('.png', '.jpg', '.jpeg'))])\n",
    "right_images = sorted([os.path.join(right_folder, f) for f in os.listdir(right_folder) if f.endswith(('.png', '.jpg', '.jpeg'))])\n",
    "\n",
    "# Ensure both folders have the same number of images\n",
    "if len(left_images) != len(right_images):\n",
    "    print(\"Error: The number of images in the left and right folders does not match.\")\n",
    "    exit()\n",
    "\n",
    "# Process each pair of images\n",
    "for left_image_path, right_image_path in zip(left_images, right_images):\n",
    "    for image_path, output_folder in [(left_image_path, output_left_folder), (right_image_path, output_right_folder)]:\n",
    "        # Read the image\n",
    "        image = cv2.imread(image_path)\n",
    "\n",
    "        # Perform inference with YOLOv5\n",
    "        results = model(image)\n",
    "\n",
    "        # Render the results on the image\n",
    "        rendered_image = results.render()[0]\n",
    "\n",
    "        # Save the processed image\n",
    "        output_path = os.path.join(output_folder, os.path.basename(image_path))\n",
    "        cv2.imwrite(output_path, rendered_image)\n",
    "        print(f\"Processed and saved: {output_path}\")\n",
    "\n",
    "print(\"Object detection completed for all images.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply YOLO v7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not open requirements file: [Errno 2] No such file or directory: 'yolov7/requirements.txt'\n"
     ]
    }
   ],
   "source": [
    "pip install -r yolov7/requirements.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1515002750.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[3], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    git clone https://github.com/WongKinYiu/yolov7.git\u001b[0m\n\u001b[1;37m        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "git clone https://github.com/WongKinYiu/yolov7.git\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'models'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpathlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DetectMultiBackend\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LoadImages\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgeneral\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m non_max_suppression, scale_coords\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'models'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from models.common import DetectMultiBackend\n",
    "from utils.datasets import LoadImages\n",
    "from utils.general import non_max_suppression, scale_coords\n",
    "from utils.plots import Annotator, colors\n",
    "\n",
    "# Paths to YOLOv7 weights and configuration\n",
    "weights_path = r\"yolov7.pt\"  # Path to the YOLOv7 weights file\n",
    "\n",
    "# Paths to the folders containing images\n",
    "left_folder = r\"C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Left5\"\n",
    "right_folder = r\"C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Right5\"\n",
    "\n",
    "# Output folders for processed images\n",
    "output_left_folder = r\"C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5_YOLOv7\"\n",
    "output_right_folder = r\"C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5_YOLOv7\"\n",
    "\n",
    "os.makedirs(output_left_folder, exist_ok=True)\n",
    "os.makedirs(output_right_folder, exist_ok=True)\n",
    "\n",
    "# Load YOLOv7 model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = DetectMultiBackend(weights_path, device=device)\n",
    "stride, names, pt = model.stride, model.names, model.pt\n",
    "\n",
    "# Process each folder\n",
    "image_size = 640  # YOLOv7 inference image size\n",
    "conf_thres = 0.25  # Confidence threshold\n",
    "iou_thres = 0.45  # IoU threshold for NMS\n",
    "\n",
    "# Get sorted list of image files\n",
    "left_images = sorted([os.path.join(left_folder, f) for f in os.listdir(left_folder) if f.endswith(('.png', '.jpg', '.jpeg'))])\n",
    "right_images = sorted([os.path.join(right_folder, f) for f in os.listdir(right_folder) if f.endswith(('.png', '.jpg', '.jpeg'))])\n",
    "\n",
    "# Ensure both folders have the same number of images\n",
    "if len(left_images) != len(right_images):\n",
    "    print(\"Error: The number of images in the left and right folders does not match.\")\n",
    "    exit()\n",
    "\n",
    "# Process each pair of images\n",
    "for left_image_path, right_image_path in zip(left_images, right_images):\n",
    "    for image_path, output_folder in [(left_image_path, output_left_folder), (right_image_path, output_right_folder)]:\n",
    "        # Load image\n",
    "        img = cv2.imread(image_path)\n",
    "        assert img is not None, f\"Image not found: {image_path}\"\n",
    "\n",
    "        # Preprocess image\n",
    "        img_size = (image_size, image_size)\n",
    "        img_resized = cv2.resize(img, img_size)\n",
    "        img_tensor = torch.from_numpy(img_resized).permute(2, 0, 1).unsqueeze(0).to(device)\n",
    "        img_tensor = img_tensor.float() / 255.0  # Normalize to [0, 1]\n",
    "\n",
    "        # Run YOLOv7 inference\n",
    "        pred = model(img_tensor, augment=False, visualize=False)\n",
    "        pred = non_max_suppression(pred, conf_thres, iou_thres, classes=None, agnostic=False)\n",
    "\n",
    "        # Draw bounding boxes on the image\n",
    "        annotator = Annotator(img, line_width=2, example=str(names))\n",
    "        for det in pred:\n",
    "            if len(det):\n",
    "                # Rescale boxes\n",
    "                det[:, :4] = scale_coords(img_tensor.shape[2:], det[:, :4], img.shape).round()\n",
    "                for *xyxy, conf, cls in det:\n",
    "                    label = f\"{names[int(cls)]} {conf:.2f}\"\n",
    "                    annotator.box_label(xyxy, label, color=colors(int(cls), True))\n",
    "\n",
    "        # Save the processed image\n",
    "        output_path = os.path.join(output_folder, os.path.basename(image_path))\n",
    "        #cv2.imwrite(output_path, img)\n",
    "        print(f\"Processed and saved: {output_path}\")\n",
    "\n",
    "print(\"Object detection completed for all images.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply YOLO v8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8s.pt to 'yolov8s.pt'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21.5M/21.5M [00:01<00:00, 21.7MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 (no detections), 262.2ms\n",
      "Speed: 5.3ms preprocess, 262.2ms inference, 8.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5_YOLOv8\\left_0000.jpg\n",
      "\n",
      "0: 480x640 (no detections), 249.0ms\n",
      "Speed: 2.0ms preprocess, 249.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5_YOLOv8\\right_0000.jpg\n",
      "\n",
      "0: 480x640 (no detections), 221.9ms\n",
      "Speed: 2.0ms preprocess, 221.9ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5_YOLOv8\\left_0001.jpg\n",
      "\n",
      "0: 480x640 1 chair, 192.4ms\n",
      "Speed: 2.0ms preprocess, 192.4ms inference, 4.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5_YOLOv8\\right_0001.jpg\n",
      "\n",
      "0: 480x640 1 chair, 214.9ms\n",
      "Speed: 5.5ms preprocess, 214.9ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5_YOLOv8\\left_0002.jpg\n",
      "\n",
      "0: 480x640 1 person, 1 chair, 221.9ms\n",
      "Speed: 3.7ms preprocess, 221.9ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5_YOLOv8\\right_0002.jpg\n",
      "\n",
      "0: 480x640 1 chair, 226.6ms\n",
      "Speed: 4.0ms preprocess, 226.6ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5_YOLOv8\\left_0003.jpg\n",
      "\n",
      "0: 480x640 1 person, 1 chair, 204.2ms\n",
      "Speed: 1.5ms preprocess, 204.2ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5_YOLOv8\\right_0003.jpg\n",
      "\n",
      "0: 480x640 1 chair, 227.0ms\n",
      "Speed: 3.0ms preprocess, 227.0ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5_YOLOv8\\left_0004.jpg\n",
      "\n",
      "0: 480x640 1 person, 1 chair, 232.5ms\n",
      "Speed: 2.0ms preprocess, 232.5ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5_YOLOv8\\right_0004.jpg\n",
      "\n",
      "0: 480x640 1 chair, 229.1ms\n",
      "Speed: 2.0ms preprocess, 229.1ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5_YOLOv8\\left_0005.jpg\n",
      "\n",
      "0: 480x640 1 person, 1 chair, 202.7ms\n",
      "Speed: 2.3ms preprocess, 202.7ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5_YOLOv8\\right_0005.jpg\n",
      "\n",
      "0: 480x640 1 chair, 213.9ms\n",
      "Speed: 2.0ms preprocess, 213.9ms inference, 2.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5_YOLOv8\\left_0006.jpg\n",
      "\n",
      "0: 480x640 1 person, 1 chair, 274.0ms\n",
      "Speed: 2.5ms preprocess, 274.0ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5_YOLOv8\\right_0006.jpg\n",
      "\n",
      "0: 480x640 1 chair, 176.3ms\n",
      "Speed: 2.1ms preprocess, 176.3ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5_YOLOv8\\left_0007.jpg\n",
      "\n",
      "0: 480x640 1 person, 1 chair, 214.4ms\n",
      "Speed: 2.0ms preprocess, 214.4ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5_YOLOv8\\right_0007.jpg\n",
      "\n",
      "0: 480x640 1 chair, 227.2ms\n",
      "Speed: 2.4ms preprocess, 227.2ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5_YOLOv8\\left_0008.jpg\n",
      "\n",
      "0: 480x640 1 person, 1 cup, 1 chair, 200.6ms\n",
      "Speed: 4.1ms preprocess, 200.6ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5_YOLOv8\\right_0008.jpg\n",
      "\n",
      "0: 480x640 1 chair, 276.5ms\n",
      "Speed: 20.5ms preprocess, 276.5ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5_YOLOv8\\left_0009.jpg\n",
      "\n",
      "0: 480x640 1 person, 1 chair, 1 bed, 221.9ms\n",
      "Speed: 2.0ms preprocess, 221.9ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5_YOLOv8\\right_0009.jpg\n",
      "\n",
      "0: 480x640 1 chair, 180.0ms\n",
      "Speed: 4.1ms preprocess, 180.0ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5_YOLOv8\\left_0010.jpg\n",
      "\n",
      "0: 480x640 1 person, 1 chair, 205.9ms\n",
      "Speed: 1.4ms preprocess, 205.9ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5_YOLOv8\\right_0010.jpg\n",
      "\n",
      "0: 480x640 (no detections), 194.9ms\n",
      "Speed: 1.1ms preprocess, 194.9ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5_YOLOv8\\left_0011.jpg\n",
      "\n",
      "0: 480x640 1 chair, 215.8ms\n",
      "Speed: 1.0ms preprocess, 215.8ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5_YOLOv8\\right_0011.jpg\n",
      "\n",
      "0: 480x640 1 chair, 1 bed, 247.8ms\n",
      "Speed: 4.5ms preprocess, 247.8ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5_YOLOv8\\left_0012.jpg\n",
      "\n",
      "0: 480x640 1 cup, 1 chair, 184.6ms\n",
      "Speed: 3.0ms preprocess, 184.6ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5_YOLOv8\\right_0012.jpg\n",
      "\n",
      "0: 480x640 (no detections), 159.0ms\n",
      "Speed: 2.6ms preprocess, 159.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5_YOLOv8\\left_0013.jpg\n",
      "\n",
      "0: 480x640 (no detections), 167.4ms\n",
      "Speed: 2.0ms preprocess, 167.4ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5_YOLOv8\\right_0013.jpg\n",
      "\n",
      "0: 480x640 (no detections), 221.4ms\n",
      "Speed: 2.3ms preprocess, 221.4ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5_YOLOv8\\left_0014.jpg\n",
      "\n",
      "0: 480x640 1 chair, 194.3ms\n",
      "Speed: 2.1ms preprocess, 194.3ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5_YOLOv8\\right_0014.jpg\n",
      "\n",
      "0: 480x640 1 cup, 191.7ms\n",
      "Speed: 2.0ms preprocess, 191.7ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5_YOLOv8\\left_0015.jpg\n",
      "\n",
      "0: 480x640 1 cup, 1 chair, 176.1ms\n",
      "Speed: 3.0ms preprocess, 176.1ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5_YOLOv8\\right_0015.jpg\n",
      "\n",
      "0: 480x640 1 bed, 195.6ms\n",
      "Speed: 2.0ms preprocess, 195.6ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5_YOLOv8\\left_0016.jpg\n",
      "\n",
      "0: 480x640 1 chair, 1 tv, 236.1ms\n",
      "Speed: 2.2ms preprocess, 236.1ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5_YOLOv8\\right_0016.jpg\n",
      "\n",
      "0: 480x640 (no detections), 190.6ms\n",
      "Speed: 5.5ms preprocess, 190.6ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5_YOLOv8\\left_0017.jpg\n",
      "\n",
      "0: 480x640 1 chair, 180.7ms\n",
      "Speed: 2.0ms preprocess, 180.7ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5_YOLOv8\\right_0017.jpg\n",
      "\n",
      "0: 480x640 1 bed, 168.7ms\n",
      "Speed: 2.0ms preprocess, 168.7ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5_YOLOv8\\left_0018.jpg\n",
      "\n",
      "0: 480x640 1 chair, 166.8ms\n",
      "Speed: 1.0ms preprocess, 166.8ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5_YOLOv8\\right_0018.jpg\n",
      "\n",
      "0: 480x640 1 bed, 166.8ms\n",
      "Speed: 2.4ms preprocess, 166.8ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5_YOLOv8\\left_0019.jpg\n",
      "\n",
      "0: 480x640 (no detections), 164.9ms\n",
      "Speed: 1.0ms preprocess, 164.9ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5_YOLOv8\\right_0019.jpg\n",
      "\n",
      "0: 480x640 1 bed, 166.6ms\n",
      "Speed: 1.0ms preprocess, 166.6ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5_YOLOv8\\left_0020.jpg\n",
      "\n",
      "0: 480x640 1 chair, 263.8ms\n",
      "Speed: 1.0ms preprocess, 263.8ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5_YOLOv8\\right_0020.jpg\n",
      "\n",
      "0: 480x640 1 bed, 185.4ms\n",
      "Speed: 1.0ms preprocess, 185.4ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5_YOLOv8\\left_0021.jpg\n",
      "\n",
      "0: 480x640 (no detections), 261.9ms\n",
      "Speed: 2.3ms preprocess, 261.9ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5_YOLOv8\\right_0021.jpg\n",
      "\n",
      "0: 480x640 1 bed, 211.9ms\n",
      "Speed: 2.0ms preprocess, 211.9ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5_YOLOv8\\left_0022.jpg\n",
      "\n",
      "0: 480x640 (no detections), 253.4ms\n",
      "Speed: 3.3ms preprocess, 253.4ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5_YOLOv8\\right_0022.jpg\n",
      "\n",
      "0: 480x640 1 bed, 228.9ms\n",
      "Speed: 2.0ms preprocess, 228.9ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5_YOLOv8\\left_0023.jpg\n",
      "\n",
      "0: 480x640 1 cup, 217.4ms\n",
      "Speed: 1.0ms preprocess, 217.4ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5_YOLOv8\\right_0023.jpg\n",
      "\n",
      "0: 480x640 1 bed, 228.4ms\n",
      "Speed: 2.2ms preprocess, 228.4ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5_YOLOv8\\left_0024.jpg\n",
      "\n",
      "0: 480x640 1 backpack, 1 cup, 332.0ms\n",
      "Speed: 2.1ms preprocess, 332.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5_YOLOv8\\right_0024.jpg\n",
      "\n",
      "0: 480x640 1 bed, 277.4ms\n",
      "Speed: 2.0ms preprocess, 277.4ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5_YOLOv8\\left_0025.jpg\n",
      "\n",
      "0: 480x640 1 cup, 274.5ms\n",
      "Speed: 4.0ms preprocess, 274.5ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5_YOLOv8\\right_0025.jpg\n",
      "\n",
      "0: 480x640 (no detections), 248.3ms\n",
      "Speed: 3.0ms preprocess, 248.3ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5_YOLOv8\\left_0026.jpg\n",
      "\n",
      "0: 480x640 1 cup, 197.2ms\n",
      "Speed: 2.2ms preprocess, 197.2ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5_YOLOv8\\right_0026.jpg\n",
      "\n",
      "0: 480x640 1 bed, 265.4ms\n",
      "Speed: 2.0ms preprocess, 265.4ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5_YOLOv8\\left_0027.jpg\n",
      "\n",
      "0: 480x640 1 bed, 249.7ms\n",
      "Speed: 1.6ms preprocess, 249.7ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5_YOLOv8\\right_0027.jpg\n",
      "\n",
      "0: 480x640 (no detections), 228.5ms\n",
      "Speed: 1.0ms preprocess, 228.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5_YOLOv8\\left_0028.jpg\n",
      "\n",
      "0: 480x640 1 bed, 188.7ms\n",
      "Speed: 2.1ms preprocess, 188.7ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5_YOLOv8\\right_0028.jpg\n",
      "\n",
      "0: 480x640 1 bed, 195.1ms\n",
      "Speed: 4.0ms preprocess, 195.1ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5_YOLOv8\\left_0029.jpg\n",
      "\n",
      "0: 480x640 1 bed, 202.8ms\n",
      "Speed: 3.0ms preprocess, 202.8ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5_YOLOv8\\right_0029.jpg\n",
      "\n",
      "0: 480x640 1 bed, 296.4ms\n",
      "Speed: 3.2ms preprocess, 296.4ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5_YOLOv8\\left_0030.jpg\n",
      "\n",
      "0: 480x640 1 bed, 213.0ms\n",
      "Speed: 3.1ms preprocess, 213.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5_YOLOv8\\right_0030.jpg\n",
      "\n",
      "0: 480x640 (no detections), 185.6ms\n",
      "Speed: 2.0ms preprocess, 185.6ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5_YOLOv8\\left_0031.jpg\n",
      "\n",
      "0: 480x640 (no detections), 232.7ms\n",
      "Speed: 3.1ms preprocess, 232.7ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5_YOLOv8\\right_0031.jpg\n",
      "\n",
      "0: 480x640 (no detections), 252.1ms\n",
      "Speed: 5.7ms preprocess, 252.1ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5_YOLOv8\\left_0032.jpg\n",
      "\n",
      "0: 480x640 (no detections), 261.7ms\n",
      "Speed: 2.0ms preprocess, 261.7ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5_YOLOv8\\right_0032.jpg\n",
      "\n",
      "0: 480x640 1 sports ball, 253.4ms\n",
      "Speed: 7.4ms preprocess, 253.4ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5_YOLOv8\\left_0033.jpg\n",
      "\n",
      "0: 480x640 (no detections), 229.0ms\n",
      "Speed: 3.0ms preprocess, 229.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5_YOLOv8\\right_0033.jpg\n",
      "\n",
      "0: 480x640 1 teddy bear, 234.5ms\n",
      "Speed: 2.5ms preprocess, 234.5ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5_YOLOv8\\left_0034.jpg\n",
      "\n",
      "0: 480x640 (no detections), 220.9ms\n",
      "Speed: 3.0ms preprocess, 220.9ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5_YOLOv8\\right_0034.jpg\n",
      "\n",
      "0: 480x640 (no detections), 196.8ms\n",
      "Speed: 2.2ms preprocess, 196.8ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5_YOLOv8\\left_0035.jpg\n",
      "\n",
      "0: 480x640 1 teddy bear, 214.0ms\n",
      "Speed: 2.0ms preprocess, 214.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5_YOLOv8\\right_0035.jpg\n",
      "\n",
      "0: 480x640 (no detections), 243.4ms\n",
      "Speed: 4.0ms preprocess, 243.4ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5_YOLOv8\\left_0036.jpg\n",
      "\n",
      "0: 480x640 (no detections), 223.1ms\n",
      "Speed: 3.7ms preprocess, 223.1ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5_YOLOv8\\right_0036.jpg\n",
      "\n",
      "0: 480x640 (no detections), 236.0ms\n",
      "Speed: 3.0ms preprocess, 236.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5_YOLOv8\\left_0037.jpg\n",
      "\n",
      "0: 480x640 (no detections), 204.6ms\n",
      "Speed: 3.1ms preprocess, 204.6ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5_YOLOv8\\right_0037.jpg\n",
      "\n",
      "0: 480x640 (no detections), 189.0ms\n",
      "Speed: 1.0ms preprocess, 189.0ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5_YOLOv8\\left_0038.jpg\n",
      "\n",
      "0: 480x640 1 teddy bear, 193.5ms\n",
      "Speed: 2.0ms preprocess, 193.5ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5_YOLOv8\\right_0038.jpg\n",
      "\n",
      "0: 480x640 (no detections), 199.5ms\n",
      "Speed: 2.4ms preprocess, 199.5ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5_YOLOv8\\left_0039.jpg\n",
      "\n",
      "0: 480x640 (no detections), 292.1ms\n",
      "Speed: 2.0ms preprocess, 292.1ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5_YOLOv8\\right_0039.jpg\n",
      "\n",
      "0: 480x640 (no detections), 231.1ms\n",
      "Speed: 4.5ms preprocess, 231.1ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5_YOLOv8\\left_0040.jpg\n",
      "\n",
      "0: 480x640 (no detections), 210.9ms\n",
      "Speed: 2.0ms preprocess, 210.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5_YOLOv8\\right_0040.jpg\n",
      "\n",
      "0: 480x640 (no detections), 284.4ms\n",
      "Speed: 4.0ms preprocess, 284.4ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5_YOLOv8\\left_0041.jpg\n",
      "\n",
      "0: 480x640 (no detections), 346.3ms\n",
      "Speed: 3.0ms preprocess, 346.3ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5_YOLOv8\\right_0041.jpg\n",
      "\n",
      "0: 480x640 (no detections), 279.7ms\n",
      "Speed: 2.0ms preprocess, 279.7ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5_YOLOv8\\left_0042.jpg\n",
      "\n",
      "0: 480x640 (no detections), 227.8ms\n",
      "Speed: 2.0ms preprocess, 227.8ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5_YOLOv8\\right_0042.jpg\n",
      "\n",
      "0: 480x640 (no detections), 304.2ms\n",
      "Speed: 1.0ms preprocess, 304.2ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5_YOLOv8\\left_0043.jpg\n",
      "\n",
      "0: 480x640 (no detections), 291.3ms\n",
      "Speed: 2.0ms preprocess, 291.3ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5_YOLOv8\\right_0043.jpg\n",
      "\n",
      "0: 480x640 (no detections), 265.9ms\n",
      "Speed: 2.1ms preprocess, 265.9ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5_YOLOv8\\left_0044.jpg\n",
      "\n",
      "0: 480x640 (no detections), 236.3ms\n",
      "Speed: 5.5ms preprocess, 236.3ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5_YOLOv8\\right_0044.jpg\n",
      "\n",
      "0: 480x640 (no detections), 235.0ms\n",
      "Speed: 5.5ms preprocess, 235.0ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5_YOLOv8\\left_0045.jpg\n",
      "\n",
      "0: 480x640 (no detections), 229.3ms\n",
      "Speed: 4.5ms preprocess, 229.3ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5_YOLOv8\\right_0045.jpg\n",
      "\n",
      "0: 480x640 (no detections), 213.9ms\n",
      "Speed: 2.0ms preprocess, 213.9ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5_YOLOv8\\left_0046.jpg\n",
      "\n",
      "0: 480x640 (no detections), 371.3ms\n",
      "Speed: 2.0ms preprocess, 371.3ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5_YOLOv8\\right_0046.jpg\n",
      "\n",
      "0: 480x640 (no detections), 213.1ms\n",
      "Speed: 2.0ms preprocess, 213.1ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5_YOLOv8\\left_0047.jpg\n",
      "\n",
      "0: 480x640 (no detections), 249.4ms\n",
      "Speed: 1.0ms preprocess, 249.4ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5_YOLOv8\\right_0047.jpg\n",
      "\n",
      "0: 480x640 (no detections), 284.9ms\n",
      "Speed: 4.0ms preprocess, 284.9ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5_YOLOv8\\left_0048.jpg\n",
      "\n",
      "0: 480x640 (no detections), 259.6ms\n",
      "Speed: 2.3ms preprocess, 259.6ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5_YOLOv8\\right_0048.jpg\n",
      "\n",
      "0: 480x640 (no detections), 261.0ms\n",
      "Speed: 2.0ms preprocess, 261.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5_YOLOv8\\left_0049.jpg\n",
      "\n",
      "0: 480x640 (no detections), 213.8ms\n",
      "Speed: 2.0ms preprocess, 213.8ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5_YOLOv8\\right_0049.jpg\n",
      "\n",
      "0: 480x640 2 chairs, 287.8ms\n",
      "Speed: 2.1ms preprocess, 287.8ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5_YOLOv8\\left_0050.jpg\n",
      "\n",
      "0: 480x640 2 chairs, 242.3ms\n",
      "Speed: 6.0ms preprocess, 242.3ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5_YOLOv8\\right_0050.jpg\n",
      "\n",
      "0: 480x640 2 chairs, 240.7ms\n",
      "Speed: 4.0ms preprocess, 240.7ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5_YOLOv8\\left_0051.jpg\n",
      "\n",
      "0: 480x640 2 chairs, 223.2ms\n",
      "Speed: 2.1ms preprocess, 223.2ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5_YOLOv8\\right_0051.jpg\n",
      "\n",
      "0: 480x640 (no detections), 231.2ms\n",
      "Speed: 3.0ms preprocess, 231.2ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5_YOLOv8\\left_0052.jpg\n",
      "\n",
      "0: 480x640 (no detections), 204.0ms\n",
      "Speed: 2.0ms preprocess, 204.0ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5_YOLOv8\\right_0052.jpg\n",
      "\n",
      "0: 480x640 2 chairs, 219.3ms\n",
      "Speed: 5.2ms preprocess, 219.3ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5_YOLOv8\\left_0053.jpg\n",
      "\n",
      "0: 480x640 2 chairs, 321.6ms\n",
      "Speed: 3.0ms preprocess, 321.6ms inference, 5.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5_YOLOv8\\right_0053.jpg\n",
      "\n",
      "0: 480x640 1 person, 1 chair, 373.9ms\n",
      "Speed: 2.0ms preprocess, 373.9ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5_YOLOv8\\left_0054.jpg\n",
      "\n",
      "0: 480x640 (no detections), 274.0ms\n",
      "Speed: 5.0ms preprocess, 274.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5_YOLOv8\\right_0054.jpg\n",
      "\n",
      "0: 480x640 2 chairs, 309.8ms\n",
      "Speed: 2.0ms preprocess, 309.8ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5_YOLOv8\\left_0055.jpg\n",
      "\n",
      "0: 480x640 (no detections), 243.7ms\n",
      "Speed: 2.0ms preprocess, 243.7ms inference, 0.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5_YOLOv8\\right_0055.jpg\n",
      "\n",
      "0: 480x640 2 chairs, 255.7ms\n",
      "Speed: 2.1ms preprocess, 255.7ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5_YOLOv8\\left_0056.jpg\n",
      "\n",
      "0: 480x640 (no detections), 221.9ms\n",
      "Speed: 3.0ms preprocess, 221.9ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5_YOLOv8\\right_0056.jpg\n",
      "\n",
      "0: 480x640 1 person, 2 chairs, 278.5ms\n",
      "Speed: 4.0ms preprocess, 278.5ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5_YOLOv8\\left_0057.jpg\n",
      "\n",
      "0: 480x640 1 chair, 238.1ms\n",
      "Speed: 2.8ms preprocess, 238.1ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5_YOLOv8\\right_0057.jpg\n",
      "\n",
      "0: 480x640 1 horse, 2 chairs, 221.2ms\n",
      "Speed: 2.2ms preprocess, 221.2ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5_YOLOv8\\left_0058.jpg\n",
      "\n",
      "0: 480x640 2 chairs, 219.4ms\n",
      "Speed: 3.0ms preprocess, 219.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5_YOLOv8\\right_0058.jpg\n",
      "\n",
      "0: 480x640 1 horse, 2 chairs, 214.8ms\n",
      "Speed: 3.2ms preprocess, 214.8ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5_YOLOv8\\left_0059.jpg\n",
      "\n",
      "0: 480x640 1 person, 1 chair, 220.2ms\n",
      "Speed: 2.0ms preprocess, 220.2ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5_YOLOv8\\right_0059.jpg\n",
      "\n",
      "0: 480x640 1 horse, 2 chairs, 193.5ms\n",
      "Speed: 1.0ms preprocess, 193.5ms inference, 2.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5_YOLOv8\\left_0060.jpg\n",
      "\n",
      "0: 480x640 1 person, 1 chair, 271.5ms\n",
      "Speed: 6.0ms preprocess, 271.5ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5_YOLOv8\\right_0060.jpg\n",
      "\n",
      "0: 480x640 1 horse, 1 suitcase, 2 chairs, 231.7ms\n",
      "Speed: 2.2ms preprocess, 231.7ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5_YOLOv8\\left_0061.jpg\n",
      "\n",
      "0: 480x640 1 horse, 2 chairs, 223.4ms\n",
      "Speed: 3.0ms preprocess, 223.4ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5_YOLOv8\\right_0061.jpg\n",
      "\n",
      "0: 480x640 1 horse, 3 chairs, 323.6ms\n",
      "Speed: 2.3ms preprocess, 323.6ms inference, 5.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5_YOLOv8\\left_0062.jpg\n",
      "\n",
      "0: 480x640 1 horse, 2 chairs, 230.2ms\n",
      "Speed: 3.0ms preprocess, 230.2ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5_YOLOv8\\right_0062.jpg\n",
      "\n",
      "0: 480x640 1 horse, 1 suitcase, 3 chairs, 229.0ms\n",
      "Speed: 2.0ms preprocess, 229.0ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5_YOLOv8\\left_0063.jpg\n",
      "\n",
      "0: 480x640 1 horse, 2 chairs, 241.1ms\n",
      "Speed: 5.7ms preprocess, 241.1ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5_YOLOv8\\right_0063.jpg\n",
      "\n",
      "0: 480x640 1 horse, 3 chairs, 276.3ms\n",
      "Speed: 4.0ms preprocess, 276.3ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5_YOLOv8\\left_0064.jpg\n",
      "\n",
      "0: 480x640 1 horse, 3 chairs, 219.0ms\n",
      "Speed: 1.0ms preprocess, 219.0ms inference, 4.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5_YOLOv8\\right_0064.jpg\n",
      "\n",
      "0: 480x640 1 horse, 1 suitcase, 3 chairs, 199.0ms\n",
      "Speed: 4.0ms preprocess, 199.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5_YOLOv8\\left_0065.jpg\n",
      "\n",
      "0: 480x640 1 horse, 2 chairs, 193.6ms\n",
      "Speed: 2.0ms preprocess, 193.6ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5_YOLOv8\\right_0065.jpg\n",
      "\n",
      "0: 480x640 3 chairs, 213.7ms\n",
      "Speed: 1.0ms preprocess, 213.7ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5_YOLOv8\\left_0066.jpg\n",
      "\n",
      "0: 480x640 1 horse, 2 chairs, 262.9ms\n",
      "Speed: 5.0ms preprocess, 262.9ms inference, 0.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5_YOLOv8\\right_0066.jpg\n",
      "\n",
      "0: 480x640 1 horse, 3 chairs, 239.1ms\n",
      "Speed: 3.1ms preprocess, 239.1ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5_YOLOv8\\left_0067.jpg\n",
      "\n",
      "0: 480x640 1 horse, 1 suitcase, 2 chairs, 306.8ms\n",
      "Speed: 1.1ms preprocess, 306.8ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5_YOLOv8\\right_0067.jpg\n",
      "\n",
      "0: 480x640 1 horse, 3 chairs, 219.2ms\n",
      "Speed: 3.2ms preprocess, 219.2ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5_YOLOv8\\left_0068.jpg\n",
      "\n",
      "0: 480x640 1 horse, 1 suitcase, 3 chairs, 270.9ms\n",
      "Speed: 2.0ms preprocess, 270.9ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5_YOLOv8\\right_0068.jpg\n",
      "\n",
      "0: 480x640 1 horse, 3 chairs, 250.8ms\n",
      "Speed: 2.0ms preprocess, 250.8ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5_YOLOv8\\left_0069.jpg\n",
      "\n",
      "0: 480x640 1 horse, 2 chairs, 272.7ms\n",
      "Speed: 2.7ms preprocess, 272.7ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5_YOLOv8\\right_0069.jpg\n",
      "\n",
      "0: 480x640 1 horse, 2 chairs, 242.2ms\n",
      "Speed: 2.0ms preprocess, 242.2ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5_YOLOv8\\left_0070.jpg\n",
      "\n",
      "0: 480x640 1 horse, 2 chairs, 270.0ms\n",
      "Speed: 2.0ms preprocess, 270.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5_YOLOv8\\right_0070.jpg\n",
      "\n",
      "0: 480x640 1 horse, 1 chair, 269.7ms\n",
      "Speed: 2.2ms preprocess, 269.7ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5_YOLOv8\\left_0071.jpg\n",
      "\n",
      "0: 480x640 1 horse, 2 chairs, 217.2ms\n",
      "Speed: 2.0ms preprocess, 217.2ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5_YOLOv8\\right_0071.jpg\n",
      "\n",
      "0: 480x640 3 chairs, 253.0ms\n",
      "Speed: 3.0ms preprocess, 253.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5_YOLOv8\\left_0072.jpg\n",
      "\n",
      "0: 480x640 2 chairs, 208.7ms\n",
      "Speed: 4.3ms preprocess, 208.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5_YOLOv8\\right_0072.jpg\n",
      "\n",
      "0: 480x640 1 horse, 3 chairs, 289.2ms\n",
      "Speed: 2.0ms preprocess, 289.2ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5_YOLOv8\\left_0073.jpg\n",
      "\n",
      "0: 480x640 1 horse, 3 chairs, 316.0ms\n",
      "Speed: 4.0ms preprocess, 316.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5_YOLOv8\\right_0073.jpg\n",
      "\n",
      "0: 480x640 1 horse, 3 chairs, 252.4ms\n",
      "Speed: 2.1ms preprocess, 252.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5_YOLOv8\\left_0074.jpg\n",
      "\n",
      "0: 480x640 2 chairs, 231.8ms\n",
      "Speed: 2.2ms preprocess, 231.8ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5_YOLOv8\\right_0074.jpg\n",
      "\n",
      "0: 480x640 1 person, 1 chair, 206.2ms\n",
      "Speed: 3.0ms preprocess, 206.2ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5_YOLOv8\\left_0075.jpg\n",
      "\n",
      "0: 480x640 1 chair, 203.7ms\n",
      "Speed: 3.1ms preprocess, 203.7ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5_YOLOv8\\right_0075.jpg\n",
      "\n",
      "0: 480x640 2 chairs, 314.0ms\n",
      "Speed: 2.2ms preprocess, 314.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5_YOLOv8\\left_0076.jpg\n",
      "\n",
      "0: 480x640 1 person, 1 chair, 251.3ms\n",
      "Speed: 2.0ms preprocess, 251.3ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5_YOLOv8\\right_0076.jpg\n",
      "\n",
      "0: 480x640 1 person, 2 chairs, 262.6ms\n",
      "Speed: 7.0ms preprocess, 262.6ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5_YOLOv8\\left_0077.jpg\n",
      "\n",
      "0: 480x640 1 chair, 260.9ms\n",
      "Speed: 4.0ms preprocess, 260.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5_YOLOv8\\right_0077.jpg\n",
      "\n",
      "0: 480x640 1 person, 1 chair, 241.0ms\n",
      "Speed: 3.0ms preprocess, 241.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5_YOLOv8\\left_0078.jpg\n",
      "\n",
      "0: 480x640 1 laptop, 235.6ms\n",
      "Speed: 3.0ms preprocess, 235.6ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5_YOLOv8\\right_0078.jpg\n",
      "\n",
      "0: 480x640 1 chair, 223.2ms\n",
      "Speed: 1.0ms preprocess, 223.2ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5_YOLOv8\\left_0079.jpg\n",
      "\n",
      "0: 480x640 (no detections), 260.8ms\n",
      "Speed: 1.0ms preprocess, 260.8ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5_YOLOv8\\right_0079.jpg\n",
      "\n",
      "0: 480x640 3 chairs, 214.4ms\n",
      "Speed: 2.3ms preprocess, 214.4ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5_YOLOv8\\left_0080.jpg\n",
      "\n",
      "0: 480x640 4 chairs, 208.3ms\n",
      "Speed: 4.0ms preprocess, 208.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5_YOLOv8\\right_0080.jpg\n",
      "\n",
      "0: 480x640 2 chairs, 260.6ms\n",
      "Speed: 3.0ms preprocess, 260.6ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5_YOLOv8\\left_0081.jpg\n",
      "\n",
      "0: 480x640 (no detections), 219.7ms\n",
      "Speed: 2.0ms preprocess, 219.7ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5_YOLOv8\\right_0081.jpg\n",
      "\n",
      "0: 480x640 1 backpack, 200.1ms\n",
      "Speed: 2.0ms preprocess, 200.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5_YOLOv8\\left_0082.jpg\n",
      "\n",
      "0: 480x640 1 chair, 191.4ms\n",
      "Speed: 3.0ms preprocess, 191.4ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5_YOLOv8\\right_0082.jpg\n",
      "\n",
      "0: 480x640 1 suitcase, 314.6ms\n",
      "Speed: 3.0ms preprocess, 314.6ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5_YOLOv8\\left_0083.jpg\n",
      "\n",
      "0: 480x640 (no detections), 260.6ms\n",
      "Speed: 3.4ms preprocess, 260.6ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5_YOLOv8\\right_0083.jpg\n",
      "\n",
      "0: 480x640 1 backpack, 1 suitcase, 200.9ms\n",
      "Speed: 3.0ms preprocess, 200.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5_YOLOv8\\left_0084.jpg\n",
      "\n",
      "0: 480x640 (no detections), 223.9ms\n",
      "Speed: 2.0ms preprocess, 223.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5_YOLOv8\\right_0084.jpg\n",
      "\n",
      "0: 480x640 1 suitcase, 1 chair, 250.0ms\n",
      "Speed: 3.0ms preprocess, 250.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5_YOLOv8\\left_0085.jpg\n",
      "\n",
      "0: 480x640 1 elephant, 1 chair, 236.1ms\n",
      "Speed: 1.0ms preprocess, 236.1ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5_YOLOv8\\right_0085.jpg\n",
      "\n",
      "0: 480x640 1 suitcase, 2 chairs, 204.0ms\n",
      "Speed: 3.0ms preprocess, 204.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5_YOLOv8\\left_0086.jpg\n",
      "\n",
      "0: 480x640 (no detections), 230.9ms\n",
      "Speed: 2.0ms preprocess, 230.9ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5_YOLOv8\\right_0086.jpg\n",
      "\n",
      "0: 480x640 1 backpack, 1 suitcase, 2 chairs, 223.0ms\n",
      "Speed: 2.1ms preprocess, 223.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5_YOLOv8\\left_0087.jpg\n",
      "\n",
      "0: 480x640 1 chair, 214.2ms\n",
      "Speed: 3.1ms preprocess, 214.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5_YOLOv8\\right_0087.jpg\n",
      "\n",
      "0: 480x640 2 chairs, 189.4ms\n",
      "Speed: 2.0ms preprocess, 189.4ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5_YOLOv8\\left_0088.jpg\n",
      "\n",
      "0: 480x640 1 chair, 226.4ms\n",
      "Speed: 2.0ms preprocess, 226.4ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5_YOLOv8\\right_0088.jpg\n",
      "\n",
      "0: 480x640 1 suitcase, 1 chair, 1 clock, 222.8ms\n",
      "Speed: 2.1ms preprocess, 222.8ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5_YOLOv8\\left_0089.jpg\n",
      "\n",
      "0: 480x640 1 clock, 219.3ms\n",
      "Speed: 1.0ms preprocess, 219.3ms inference, 2.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5_YOLOv8\\right_0089.jpg\n",
      "\n",
      "0: 480x640 1 suitcase, 1 chair, 236.6ms\n",
      "Speed: 2.0ms preprocess, 236.6ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5_YOLOv8\\left_0090.jpg\n",
      "\n",
      "0: 480x640 1 elephant, 1 suitcase, 303.2ms\n",
      "Speed: 3.5ms preprocess, 303.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5_YOLOv8\\right_0090.jpg\n",
      "\n",
      "0: 480x640 1 suitcase, 223.7ms\n",
      "Speed: 3.1ms preprocess, 223.7ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5_YOLOv8\\left_0091.jpg\n",
      "\n",
      "0: 480x640 1 suitcase, 258.3ms\n",
      "Speed: 3.0ms preprocess, 258.3ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5_YOLOv8\\right_0091.jpg\n",
      "\n",
      "0: 480x640 1 suitcase, 327.8ms\n",
      "Speed: 2.2ms preprocess, 327.8ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5_YOLOv8\\left_0092.jpg\n",
      "\n",
      "0: 480x640 1 suitcase, 259.4ms\n",
      "Speed: 2.1ms preprocess, 259.4ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5_YOLOv8\\right_0092.jpg\n",
      "\n",
      "0: 480x640 1 clock, 252.2ms\n",
      "Speed: 3.0ms preprocess, 252.2ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5_YOLOv8\\left_0093.jpg\n",
      "\n",
      "0: 480x640 1 clock, 300.5ms\n",
      "Speed: 2.0ms preprocess, 300.5ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5_YOLOv8\\right_0093.jpg\n",
      "\n",
      "0: 480x640 1 suitcase, 3 chairs, 233.0ms\n",
      "Speed: 3.1ms preprocess, 233.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5_YOLOv8\\left_0094.jpg\n",
      "\n",
      "0: 480x640 1 clock, 226.1ms\n",
      "Speed: 6.0ms preprocess, 226.1ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5_YOLOv8\\right_0094.jpg\n",
      "\n",
      "0: 480x640 1 suitcase, 2 chairs, 211.9ms\n",
      "Speed: 2.1ms preprocess, 211.9ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5_YOLOv8\\left_0095.jpg\n",
      "\n",
      "0: 480x640 1 clock, 281.1ms\n",
      "Speed: 4.0ms preprocess, 281.1ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5_YOLOv8\\right_0095.jpg\n",
      "\n",
      "0: 480x640 1 suitcase, 1 clock, 230.9ms\n",
      "Speed: 2.5ms preprocess, 230.9ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5_YOLOv8\\left_0096.jpg\n",
      "\n",
      "0: 480x640 (no detections), 213.7ms\n",
      "Speed: 2.0ms preprocess, 213.7ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5_YOLOv8\\right_0096.jpg\n",
      "\n",
      "0: 480x640 (no detections), 214.2ms\n",
      "Speed: 2.2ms preprocess, 214.2ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5_YOLOv8\\left_0097.jpg\n",
      "\n",
      "0: 480x640 (no detections), 199.6ms\n",
      "Speed: 3.8ms preprocess, 199.6ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5_YOLOv8\\right_0097.jpg\n",
      "\n",
      "0: 480x640 1 bird, 1 backpack, 1 suitcase, 1 chair, 200.7ms\n",
      "Speed: 2.1ms preprocess, 200.7ms inference, 0.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5_YOLOv8\\left_0098.jpg\n",
      "\n",
      "0: 480x640 1 elephant, 1 tv, 314.9ms\n",
      "Speed: 2.0ms preprocess, 314.9ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5_YOLOv8\\right_0098.jpg\n",
      "\n",
      "0: 480x640 1 chair, 257.8ms\n",
      "Speed: 2.1ms preprocess, 257.8ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5_YOLOv8\\left_0099.jpg\n",
      "\n",
      "0: 480x640 (no detections), 219.8ms\n",
      "Speed: 2.0ms preprocess, 219.8ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5_YOLOv8\\right_0099.jpg\n",
      "\n",
      "0: 480x640 1 suitcase, 1 chair, 231.1ms\n",
      "Speed: 3.0ms preprocess, 231.1ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5_YOLOv8\\left_0100.jpg\n",
      "\n",
      "0: 480x640 (no detections), 297.8ms\n",
      "Speed: 4.0ms preprocess, 297.8ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Processed and saved: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5_YOLOv8\\right_0100.jpg\n",
      "Object detection completed for all images.\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "# Load YOLOv8 model\n",
    "model = YOLO('yolov8s.pt')  # Use yolov8s.pt for a small and fast model. You can also try yolov8n.pt for even faster performance.\n",
    "\n",
    "# Paths to the folders containing images\n",
    "left_folder = r\"C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Left5\"\n",
    "right_folder = r\"C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Right5\"\n",
    "\n",
    "# Output folders for processed images\n",
    "output_left_folder = r\"C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5_YOLOv8\"\n",
    "output_right_folder = r\"C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5_YOLOv8\"\n",
    "\n",
    "os.makedirs(output_left_folder, exist_ok=True)\n",
    "os.makedirs(output_right_folder, exist_ok=True)\n",
    "\n",
    "# Get sorted list of image files\n",
    "left_images = sorted([os.path.join(left_folder, f) for f in os.listdir(left_folder) if f.endswith(('.png', '.jpg', '.jpeg'))])\n",
    "right_images = sorted([os.path.join(right_folder, f) for f in os.listdir(right_folder) if f.endswith(('.png', '.jpg', '.jpeg'))])\n",
    "\n",
    "# Ensure both folders have the same number of images\n",
    "if len(left_images) != len(right_images):\n",
    "    print(\"Error: The number of images in the left and right folders does not match.\")\n",
    "    exit()\n",
    "\n",
    "# Process each pair of images\n",
    "for left_image_path, right_image_path in zip(left_images, right_images):\n",
    "    for image_path, output_folder in [(left_image_path, output_left_folder), (right_image_path, output_right_folder)]:\n",
    "        # Read the image\n",
    "        image = cv2.imread(image_path)\n",
    "\n",
    "        # Perform inference with YOLOv8\n",
    "        results = model.predict(source=image, conf=0.5)  # Set confidence threshold to 0.5\n",
    "\n",
    "        # Annotate the image with bounding boxes and labels\n",
    "        annotated_image = results[0].plot()\n",
    "\n",
    "        # Save the processed image\n",
    "        output_path = os.path.join(output_folder, os.path.basename(image_path))\n",
    "        cv2.imwrite(output_path, annotated_image)\n",
    "        print(f\"Processed and saved: {output_path}\")\n",
    "\n",
    "print(\"Object detection completed for all images.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save and Gen video from YOLO v8 Detection images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating video from annotated images...\n",
      "Video saved to: C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\combined_output.mp4\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "# Paths to processed image folders\n",
    "processed_left_folder = r\"C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5_YOLOv8\"\n",
    "processed_right_folder = r\"C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5_YOLOv8\"\n",
    "\n",
    "# Output video configuration\n",
    "output_video_path = r\"C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\combined_output.mp4\"\n",
    "frame_width = 1280  # Combined frame width (640x2 for side-by-side)\n",
    "frame_height = 480  # Frame height (assuming 640x480 for each image)\n",
    "fps = 30  # Frames per second\n",
    "\n",
    "# Initialize video writer\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "video_writer = cv2.VideoWriter(output_video_path, fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "# Get sorted list of processed images\n",
    "left_images = sorted([os.path.join(processed_left_folder, f) for f in os.listdir(processed_left_folder) if f.endswith(('.png', '.jpg', '.jpeg'))])\n",
    "right_images = sorted([os.path.join(processed_right_folder, f) for f in os.listdir(processed_right_folder) if f.endswith(('.png', '.jpg', '.jpeg'))])\n",
    "\n",
    "# Ensure both folders have the same number of images\n",
    "if len(left_images) != len(right_images):\n",
    "    print(\"Error: The number of images in the left and right folders does not match.\")\n",
    "    exit()\n",
    "\n",
    "# Combine and write video frames\n",
    "print(\"Creating video from annotated images...\")\n",
    "for left_image_path, right_image_path in zip(left_images, right_images):\n",
    "    # Read annotated images\n",
    "    left_image = cv2.imread(left_image_path)\n",
    "    right_image = cv2.imread(right_image_path)\n",
    "\n",
    "    if left_image is None or right_image is None:\n",
    "        print(f\"Error reading images: {left_image_path} or {right_image_path}\")\n",
    "        continue\n",
    "\n",
    "    # Resize images to ensure uniform dimensions\n",
    "    left_resized = cv2.resize(left_image, (640, 480))\n",
    "    right_resized = cv2.resize(right_image, (640, 480))\n",
    "\n",
    "    # Combine images side-by-side\n",
    "    combined_frame = cv2.hconcat([left_resized, right_resized])\n",
    "\n",
    "    # Write the combined frame to the video\n",
    "    video_writer.write(combined_frame)\n",
    "\n",
    "    # Display the combined frame (optional)\n",
    "    cv2.imshow(\"Combined Camera Feeds\", combined_frame)\n",
    "\n",
    "    # Exit on 'q' key press\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        print(\"Exiting video creation...\")\n",
    "        break\n",
    "\n",
    "# Release video writer and close OpenCV windows\n",
    "video_writer.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "print(f\"Video saved to: {output_video_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Playing video. Press 'q' to quit.\n",
      "End of video.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "# Path to the output video\n",
    "video_path = r\"C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\combined_output.mp4\"\n",
    "\n",
    "# Open the video file\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Check if the video file opened successfully\n",
    "if not cap.isOpened():\n",
    "    print(f\"Error: Unable to open video file {video_path}\")\n",
    "    exit()\n",
    "\n",
    "# Display the video\n",
    "print(\"Playing video. Press 'q' to quit.\")\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"End of video.\")\n",
    "        break\n",
    "\n",
    "    # Show the video frame\n",
    "    cv2.imshow(\"Combined Camera Feeds\", frame)\n",
    "\n",
    "    # Exit on 'q' key press\n",
    "    if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "        print(\"Exiting video playback.\")\n",
    "        break\n",
    "\n",
    "# Release the video capture object and close display windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the detection v7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualizing consecutive images. Press 'q' to exit.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "# Paths to processed image folders\n",
    "processed_left_folder = r\"C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5_YOLOv8\"\n",
    "processed_right_folder = r\"C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5_YOLOv8\"\n",
    "\n",
    "# Get sorted list of processed images\n",
    "left_images = sorted([os.path.join(processed_left_folder, f) for f in os.listdir(processed_left_folder) if f.endswith(('.png', '.jpg', '.jpeg'))])\n",
    "right_images = sorted([os.path.join(processed_right_folder, f) for f in os.listdir(processed_right_folder) if f.endswith(('.png', '.jpg', '.jpeg'))])\n",
    "\n",
    "# Ensure both folders have the same number of images\n",
    "if len(left_images) != len(right_images):\n",
    "    print(\"Error: The number of images in the left and right folders does not match.\")\n",
    "    exit()\n",
    "\n",
    "# Display consecutive images\n",
    "print(\"Visualizing consecutive images. Press 'q' to exit.\")\n",
    "for left_image_path, right_image_path in zip(left_images, right_images):\n",
    "    # Read images\n",
    "    left_image = cv2.imread(left_image_path)\n",
    "    right_image = cv2.imread(right_image_path)\n",
    "\n",
    "    if left_image is None or right_image is None:\n",
    "        print(f\"Error reading images: {left_image_path} or {right_image_path}\")\n",
    "        continue\n",
    "\n",
    "    # Resize images to ensure uniform dimensions\n",
    "    left_resized = cv2.resize(left_image, (640, 480))\n",
    "    right_resized = cv2.resize(right_image, (640, 480))\n",
    "\n",
    "    # Combine images side-by-side\n",
    "    combined_frame = cv2.hconcat([left_resized, right_resized])\n",
    "\n",
    "    # Display the combined frame\n",
    "    cv2.imshow(\"Combined Camera Feeds\", combined_frame)\n",
    "\n",
    "    # Exit on 'q' key press\n",
    "    if cv2.waitKey(100) & 0xFF == ord('q'):  # Adjust delay to control playback speed\n",
    "        print(\"Exiting visualization...\")\n",
    "        break\n",
    "\n",
    "# Close display window\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Display detected video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "# Paths to the processed image folders\n",
    "processed_left_folder = r\"C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5\"\n",
    "processed_right_folder = r\"C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5\"\n",
    "\n",
    "# Get sorted list of processed images\n",
    "processed_left_images = sorted([os.path.join(processed_left_folder, f) for f in os.listdir(processed_left_folder) if f.endswith(('.png', '.jpg', '.jpeg'))])\n",
    "processed_right_images = sorted([os.path.join(processed_right_folder, f) for f in os.listdir(processed_right_folder) if f.endswith(('.png', '.jpg', '.jpeg'))])\n",
    "\n",
    "# Display images sequentially\n",
    "for left_image_path, right_image_path in zip(processed_left_images, processed_right_images):\n",
    "    # Read images\n",
    "    left_image = cv2.imread(left_image_path)\n",
    "    right_image = cv2.imread(right_image_path)\n",
    "\n",
    "    # Combine images side by side\n",
    "    combined_frame = cv2.hconcat([left_image, right_image])\n",
    "\n",
    "    # Display the combined frame\n",
    "    cv2.imshow(\"Processed Images from Left and Right Cameras\", combined_frame)\n",
    "\n",
    "    # Press 'q' to quit the display\n",
    "    if cv2.waitKey(100) & 0xFF == ord('q'):  # Display each frame for 100ms\n",
    "        print(\"Exiting display...\")\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try Object Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing object matching...\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Paths to the processed image folders\n",
    "processed_left_folder = r\"C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Left5_YOLOv5\"\n",
    "processed_right_folder = r\"C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Processed_Right5_YOLOv5\"\n",
    "\n",
    "# Get sorted list of processed images\n",
    "processed_left_images = sorted([os.path.join(processed_left_folder, f) for f in os.listdir(processed_left_folder) if f.endswith(('.png', '.jpg', '.jpeg'))])\n",
    "processed_right_images = sorted([os.path.join(processed_right_folder, f) for f in os.listdir(processed_right_folder) if f.endswith(('.png', '.jpg', '.jpeg'))])\n",
    "\n",
    "# Function to compute IoU (Intersection over Union)\n",
    "def compute_iou(box1, box2):\n",
    "    x1, y1, w1, h1 = box1\n",
    "    x2, y2, w2, h2 = box2\n",
    "\n",
    "    xi1 = max(x1, x2)\n",
    "    yi1 = max(y1, y2)\n",
    "    xi2 = min(x1 + w1, x2 + w2)\n",
    "    yi2 = min(y1 + h1, y2 + h2)\n",
    "\n",
    "    inter_area = max(0, xi2 - xi1) * max(0, yi2 - yi1)\n",
    "    box1_area = w1 * h1\n",
    "    box2_area = w2 * h2\n",
    "\n",
    "    union_area = box1_area + box2_area - inter_area\n",
    "    return inter_area / union_area if union_area > 0 else 0\n",
    "\n",
    "# Function to parse YOLO bounding boxes from processed images\n",
    "def parse_yolo_boxes(image_path):\n",
    "    boxes = []\n",
    "    class_ids = []\n",
    "\n",
    "    # For this example, assume bounding box and class information are encoded in the image filename\n",
    "    # Alternatively, this can be read from a separate text/JSON file if YOLO writes outputs there\n",
    "    # Modify this function to suit your setup\n",
    "    return boxes, class_ids\n",
    "\n",
    "# Perform object matching\n",
    "print(\"Performing object matching...\")\n",
    "for left_image_path, right_image_path in zip(processed_left_images, processed_right_images):\n",
    "    # Read images\n",
    "    left_image = cv2.imread(left_image_path)\n",
    "    right_image = cv2.imread(right_image_path)\n",
    "\n",
    "    # Extract YOLO-detected objects\n",
    "    left_boxes, left_class_ids = parse_yolo_boxes(left_image_path)\n",
    "    right_boxes, right_class_ids = parse_yolo_boxes(right_image_path)\n",
    "\n",
    "    # Match objects based on IoU and class ID\n",
    "    matches = []\n",
    "    for i, box_left in enumerate(left_boxes):\n",
    "        best_match = -1\n",
    "        best_iou = 0\n",
    "        for j, box_right in enumerate(right_boxes):\n",
    "            if left_class_ids[i] == right_class_ids[j]:  # Match only same-class objects\n",
    "                iou = compute_iou(box_left, box_right)\n",
    "                if iou > best_iou and iou > 0.5:  # IoU threshold\n",
    "                    best_iou = iou\n",
    "                    best_match = j\n",
    "        if best_match != -1:\n",
    "            matches.append((i, best_match))\n",
    "\n",
    "    # Visualize matches\n",
    "    for i, j in matches:\n",
    "        # Draw bounding boxes on both images\n",
    "        x1, y1, w1, h1 = left_boxes[i]\n",
    "        x2, y2, w2, h2 = right_boxes[j]\n",
    "\n",
    "        cv2.rectangle(left_image, (x1, y1), (x1 + w1, y1 + h1), (0, 255, 0), 2)\n",
    "        cv2.rectangle(right_image, (x2, y2), (x2 + w2, y2 + h2), (0, 255, 0), 2)\n",
    "\n",
    "        # Draw a line connecting the matched objects\n",
    "        center_left = (x1 + w1 // 2, y1 + h1 // 2)\n",
    "        center_right = (x2 + w2 // 2, y2 + h2 // 2)\n",
    "        cv2.line(left_image, center_left, (center_right[0] + 640, center_right[1]), (255, 0, 0), 2)\n",
    "\n",
    "    # Combine images side by side\n",
    "    combined_frame = cv2.hconcat([left_image, right_image])\n",
    "\n",
    "    # Display the combined frame with matches\n",
    "    cv2.imshow(\"Matched Objects\", combined_frame)\n",
    "\n",
    "    # Wait for 'q' to quit or 100ms per frame\n",
    "    if cv2.waitKey(100) & 0xFF == ord('q'):\n",
    "        print(\"Exiting display...\")\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature in bounding box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing images with Shi-Tomasi Corner Detection and YOLO Bounding Boxes...\n",
      "Exiting display...\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Paths to YOLOv3 files\n",
    "weights_path = r\"C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\yolov3\\yolov3.weights\"\n",
    "config_path = r\"C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\yolov3\\yolov3.cfg\"\n",
    "labels_path = r\"C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\yolov3\\coco.names\"\n",
    "\n",
    "# Load YOLO model\n",
    "net = cv2.dnn.readNetFromDarknet(config_path, weights_path)\n",
    "\n",
    "# Shi-Tomasi parameters\n",
    "maxCorners = 300  # Number of corners to detect\n",
    "qualityLevel = 0.01\n",
    "minDistance = 10\n",
    "\n",
    "# YOLO detection function with NMS\n",
    "def detect_objects(image, net):\n",
    "    (H, W) = image.shape[:2]\n",
    "    blob = cv2.dnn.blobFromImage(image, 1 / 255.0, (416, 416), swapRB=True, crop=False)\n",
    "    net.setInput(blob)\n",
    "    layer_names = net.getLayerNames()\n",
    "    output_layers = [layer_names[i - 1] for i in net.getUnconnectedOutLayers()]\n",
    "    layer_outputs = net.forward(output_layers)\n",
    "\n",
    "    boxes = []\n",
    "    confidences = []\n",
    "\n",
    "    for output in layer_outputs:\n",
    "        for detection in output:\n",
    "            scores = detection[5:]\n",
    "            class_id = int(np.argmax(scores))\n",
    "            confidence = scores[class_id]\n",
    "\n",
    "            if confidence > 0.5:  # Confidence threshold\n",
    "                box = detection[0:4] * np.array([W, H, W, H])\n",
    "                (centerX, centerY, width, height) = box.astype(\"int\")\n",
    "                x = int(centerX - (width / 2))\n",
    "                y = int(centerY - (height / 2))\n",
    "                boxes.append([x, y, int(width), int(height)])\n",
    "                confidences.append(float(confidence))\n",
    "\n",
    "    # Apply Non-Maximum Suppression (NMS)\n",
    "    indices = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)  # IoU and confidence thresholds\n",
    "    filtered_boxes = [boxes[i] for i in indices]\n",
    "\n",
    "    return filtered_boxes  # Return filtered bounding boxes\n",
    "\n",
    "# Shi-Tomasi Corner Detection function\n",
    "def filter_corners_by_boxes(corners, boxes):\n",
    "    filtered_corners = []\n",
    "    for corner in corners:\n",
    "        x, y = corner.ravel()\n",
    "        for (bx, by, bw, bh) in boxes:\n",
    "            if bx <= x <= bx + bw and by <= y <= by + bh:  # Check if corner is inside the box\n",
    "                filtered_corners.append(corner)\n",
    "                break\n",
    "    return np.array(filtered_corners)\n",
    "\n",
    "# Draw bounding boxes and feature points as circles\n",
    "def draw_boxes_and_features(image, boxes, corners, color_box=(0, 255, 0), color_point=(255, 0, 0)):\n",
    "    # Draw bounding boxes\n",
    "    for (x, y, w, h) in boxes:\n",
    "        cv2.rectangle(image, (x, y), (x + w, y + h), color_box, 2)  # Green box\n",
    "\n",
    "    # Draw feature points as circles\n",
    "    if corners is not None:\n",
    "        for corner in corners:\n",
    "            x, y = map(int, corner.ravel())  # Ensure x and y are integers\n",
    "            cv2.circle(image, (x, y), 3, color_point, -1)  # Blue circle\n",
    "\n",
    "# Paths to the image folders\n",
    "left_folder = r\"C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Left5\"\n",
    "right_folder = r\"C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Right5\"\n",
    "\n",
    "# Get sorted list of images\n",
    "left_images = sorted([os.path.join(left_folder, f) for f in os.listdir(left_folder) if f.endswith(('.png', '.jpg', '.jpeg'))])\n",
    "right_images = sorted([os.path.join(right_folder, f) for f in os.listdir(right_folder) if f.endswith(('.png', '.jpg', '.jpeg'))])\n",
    "\n",
    "# Process images\n",
    "print(\"Processing images with Shi-Tomasi Corner Detection and YOLO Bounding Boxes...\")\n",
    "for idx, (left_image_path, right_image_path) in enumerate(zip(left_images, right_images)):\n",
    "    # Read images\n",
    "    left_image = cv2.imread(left_image_path)\n",
    "    right_image = cv2.imread(right_image_path)\n",
    "\n",
    "    if left_image is None or right_image is None:\n",
    "        print(f\"Error reading images: {left_image_path}, {right_image_path}\")\n",
    "        continue\n",
    "\n",
    "    # Convert images to grayscale\n",
    "    gray_left = cv2.cvtColor(left_image, cv2.COLOR_BGR2GRAY)\n",
    "    gray_right = cv2.cvtColor(right_image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Detect objects using YOLO and apply NMS\n",
    "    left_boxes = detect_objects(left_image, net)\n",
    "    right_boxes = detect_objects(right_image, net)\n",
    "\n",
    "    # Detect corners using Shi-Tomasi\n",
    "    corners_left = cv2.goodFeaturesToTrack(gray_left, maxCorners=maxCorners, qualityLevel=qualityLevel, minDistance=minDistance)\n",
    "    corners_right = cv2.goodFeaturesToTrack(gray_right, maxCorners=maxCorners, qualityLevel=qualityLevel, minDistance=minDistance)\n",
    "\n",
    "    # Filter corners inside bounding boxes\n",
    "    corners_left_filtered = filter_corners_by_boxes(corners_left, left_boxes) if corners_left is not None else []\n",
    "    corners_right_filtered = filter_corners_by_boxes(corners_right, right_boxes) if corners_right is not None else []\n",
    "\n",
    "    # Draw bounding boxes and filtered corners\n",
    "    draw_boxes_and_features(left_image, left_boxes, corners_left_filtered)\n",
    "    draw_boxes_and_features(right_image, right_boxes, corners_right_filtered)\n",
    "\n",
    "    # Combine images side by side\n",
    "    combined_frame = cv2.hconcat([left_image, right_image])\n",
    "\n",
    "    # Display the combined frame\n",
    "    cv2.imshow(\"Corners within YOLO Bounding Boxes\", combined_frame)\n",
    "\n",
    "    # Wait for 'q' to quit or 100ms per frame\n",
    "    if cv2.waitKey(100) & 0xFF == ord('q'):\n",
    "        print(\"Exiting display...\")\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### START"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing images with YOLO, Shi-Tomasi, and Matching Lines...\n",
      "Exiting display...\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Paths to YOLOv3 files\n",
    "weights_path = r\"C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\yolov3\\yolov3.weights\"\n",
    "config_path = r\"C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\yolov3\\yolov3.cfg\"\n",
    "labels_path = r\"C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\yolov3\\coco.names\"\n",
    "\n",
    "# Load YOLO model\n",
    "net = cv2.dnn.readNetFromDarknet(config_path, weights_path)\n",
    "\n",
    "# Shi-Tomasi parameters\n",
    "maxCorners = 300\n",
    "qualityLevel = 0.01\n",
    "minDistance = 10\n",
    "\n",
    "# YOLO detection function with NMS\n",
    "def detect_objects(image, net):\n",
    "    (H, W) = image.shape[:2]\n",
    "    blob = cv2.dnn.blobFromImage(image, 1 / 255.0, (416, 416), swapRB=True, crop=False)\n",
    "    net.setInput(blob)\n",
    "    layer_names = net.getLayerNames()\n",
    "    output_layers = [layer_names[i - 1] for i in net.getUnconnectedOutLayers()]\n",
    "    layer_outputs = net.forward(output_layers)\n",
    "\n",
    "    boxes = []\n",
    "    confidences = []\n",
    "\n",
    "    for output in layer_outputs:\n",
    "        for detection in output:\n",
    "            scores = detection[5:]\n",
    "            class_id = int(np.argmax(scores))\n",
    "            confidence = scores[class_id]\n",
    "\n",
    "            if confidence > 0.5:  # Confidence threshold\n",
    "                box = detection[0:4] * np.array([W, H, W, H])\n",
    "                (centerX, centerY, width, height) = box.astype(\"int\")\n",
    "                x = int(centerX - (width / 2))\n",
    "                y = int(centerY - (height / 2))\n",
    "                boxes.append([x, y, int(width), int(height)])\n",
    "                confidences.append(float(confidence))\n",
    "\n",
    "    # Apply Non-Maximum Suppression (NMS)\n",
    "    indices = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)  # IoU and confidence thresholds\n",
    "    filtered_boxes = [boxes[i] for i in indices]\n",
    "\n",
    "    return filtered_boxes  # Return filtered bounding boxes\n",
    "\n",
    "# Filter corners by bounding boxes\n",
    "def filter_corners_by_boxes(corners, boxes):\n",
    "    filtered_corners = []\n",
    "    for corner in corners:\n",
    "        x, y = corner.ravel()\n",
    "        for (bx, by, bw, bh) in boxes:\n",
    "            if bx <= x <= bx + bw and by <= y <= by + bh:  # Check if corner is inside the box\n",
    "                filtered_corners.append(corner)\n",
    "                break\n",
    "    return np.array(filtered_corners)\n",
    "\n",
    "# Match features between two images for corresponding bounding boxes\n",
    "def match_features_in_boxes(image1, image2, corners1, corners2):\n",
    "    # Convert corners to keypoints\n",
    "    keypoints1 = [cv2.KeyPoint(c[0][0], c[0][1], 1) for c in corners1]\n",
    "    keypoints2 = [cv2.KeyPoint(c[0][0], c[0][1], 1) for c in corners2]\n",
    "\n",
    "    # Use ORB to compute descriptors\n",
    "    orb = cv2.ORB_create()\n",
    "    keypoints1, descriptors1 = orb.compute(image1, keypoints1)\n",
    "    keypoints2, descriptors2 = orb.compute(image2, keypoints2)\n",
    "\n",
    "    # Match features using Brute-Force matcher\n",
    "    bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "    matches = bf.match(descriptors1, descriptors2)\n",
    "\n",
    "    # Sort matches by distance\n",
    "    matches = sorted(matches, key=lambda x: x.distance)\n",
    "\n",
    "    return keypoints1, keypoints2, matches\n",
    "\n",
    "# Draw bounding boxes and matching lines\n",
    "def draw_boxes_and_matches(image1, image2, boxes1, boxes2, keypoints1, keypoints2, matches):\n",
    "    combined_frame = cv2.hconcat([image1, image2])\n",
    "\n",
    "    # Draw bounding boxes\n",
    "    for (x, y, w, h) in boxes1:\n",
    "        cv2.rectangle(image1, (x, y), (x + w, y + h), (0, 255, 0), 2)  # Green box for left\n",
    "    for (x, y, w, h) in boxes2:\n",
    "        cv2.rectangle(image2, (x, y), (x + w, y + h), (0, 255, 0), 2)  # Green box for right\n",
    "\n",
    "    # Draw matches\n",
    "    for match in matches:\n",
    "        pt1 = tuple(map(int, keypoints1[match.queryIdx].pt))  # Feature in left image\n",
    "        pt2 = tuple(map(int, keypoints2[match.trainIdx].pt))  # Feature in right image\n",
    "        pt2_shifted = (pt2[0] + image1.shape[1], pt2[1])  # Adjust for side-by-side alignment\n",
    "\n",
    "        # Draw the matching line\n",
    "        cv2.line(combined_frame, pt1, pt2_shifted, (255, 0, 0), 1)  # Blue line for matches\n",
    "\n",
    "    return combined_frame\n",
    "\n",
    "# Paths to the image folders\n",
    "left_folder = r\"C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Left1\"\n",
    "right_folder = r\"C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Right1\"\n",
    "\n",
    "# Get sorted list of images\n",
    "left_images = sorted([os.path.join(left_folder, f) for f in os.listdir(left_folder) if f.endswith(('.png', '.jpg', '.jpeg'))])\n",
    "right_images = sorted([os.path.join(right_folder, f) for f in os.listdir(right_folder) if f.endswith(('.png', '.jpg', '.jpeg'))])\n",
    "\n",
    "# Process images\n",
    "print(\"Processing images with YOLO, Shi-Tomasi, and Matching Lines...\")\n",
    "for idx, (left_image_path, right_image_path) in enumerate(zip(left_images, right_images)):\n",
    "    # Read images\n",
    "    left_image = cv2.imread(left_image_path)\n",
    "    right_image = cv2.imread(right_image_path)\n",
    "\n",
    "    if left_image is None or right_image is None:\n",
    "        print(f\"Error reading images: {left_image_path}, {right_image_path}\")\n",
    "        continue\n",
    "\n",
    "    # Convert images to grayscale\n",
    "    gray_left = cv2.cvtColor(left_image, cv2.COLOR_BGR2GRAY)\n",
    "    gray_right = cv2.cvtColor(right_image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Detect objects using YOLO and apply NMS\n",
    "    left_boxes = detect_objects(left_image, net)\n",
    "    right_boxes = detect_objects(right_image, net)\n",
    "\n",
    "    # Detect corners using Shi-Tomasi\n",
    "    corners_left = cv2.goodFeaturesToTrack(gray_left, maxCorners=maxCorners, qualityLevel=qualityLevel, minDistance=minDistance)\n",
    "    corners_right = cv2.goodFeaturesToTrack(gray_right, maxCorners=maxCorners, qualityLevel=qualityLevel, minDistance=minDistance)\n",
    "\n",
    "    # Filter corners inside bounding boxes\n",
    "    corners_left_filtered = filter_corners_by_boxes(corners_left, left_boxes) if corners_left is not None else []\n",
    "    corners_right_filtered = filter_corners_by_boxes(corners_right, right_boxes) if corners_right is not None else []\n",
    "\n",
    "    # Match features inside bounding boxes\n",
    "    keypoints_left, keypoints_right, matches = match_features_in_boxes(gray_left, gray_right, corners_left_filtered, corners_right_filtered)\n",
    "\n",
    "    # Draw bounding boxes and matches\n",
    "    matched_frame = draw_boxes_and_matches(left_image, right_image, left_boxes, right_boxes, keypoints_left, keypoints_right, matches)\n",
    "\n",
    "    # Display the matched frame\n",
    "    cv2.imshow(\"YOLO Bounding Boxes with Feature Matches\", matched_frame)\n",
    "\n",
    "    # Wait for 'q' to quit or 100ms per frame\n",
    "    if cv2.waitKey(100) & 0xFF == ord('q'):\n",
    "        print(\"Exiting display...\")\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# >>*YOLOv3 COMPLETED* MATCHING IN BOUNDING BOX "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing images with YOLO, Shi-Tomasi, and Matching Lines...\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.10.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\core\\src\\batch_distance.cpp:275: error: (-215:Assertion failed) type == src2.type() && src1.cols == src2.cols && (type == CV_32F || type == CV_8U) in function 'cv::batchDistance'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 139\u001b[0m\n\u001b[0;32m    136\u001b[0m corners_right_filtered \u001b[38;5;241m=\u001b[39m filter_corners_by_boxes(corners_right, right_boxes) \u001b[38;5;28;01mif\u001b[39;00m corners_right \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m []\n\u001b[0;32m    138\u001b[0m \u001b[38;5;66;03m# Match features inside bounding boxes\u001b[39;00m\n\u001b[1;32m--> 139\u001b[0m keypoints_left, keypoints_right, matches \u001b[38;5;241m=\u001b[39m match_features_in_boxes(gray_left, gray_right, corners_left_filtered, corners_right_filtered)\n\u001b[0;32m    141\u001b[0m \u001b[38;5;66;03m# Draw bounding boxes and matches\u001b[39;00m\n\u001b[0;32m    142\u001b[0m matched_frame \u001b[38;5;241m=\u001b[39m draw_boxes_and_matches(left_image, right_image, left_boxes, right_boxes, keypoints_left, keypoints_right, matches)\n",
      "Cell \u001b[1;32mIn[3], line 74\u001b[0m, in \u001b[0;36mmatch_features_in_boxes\u001b[1;34m(image1, image2, corners1, corners2)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;66;03m# Match features using Brute-Force matcher\u001b[39;00m\n\u001b[0;32m     73\u001b[0m bf \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mBFMatcher(cv2\u001b[38;5;241m.\u001b[39mNORM_HAMMING, crossCheck\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 74\u001b[0m matches \u001b[38;5;241m=\u001b[39m bf\u001b[38;5;241m.\u001b[39mmatch(descriptors1, descriptors2)\n\u001b[0;32m     76\u001b[0m \u001b[38;5;66;03m# Sort matches by distance\u001b[39;00m\n\u001b[0;32m     77\u001b[0m matches \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(matches, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: x\u001b[38;5;241m.\u001b[39mdistance)\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.10.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\core\\src\\batch_distance.cpp:275: error: (-215:Assertion failed) type == src2.type() && src1.cols == src2.cols && (type == CV_32F || type == CV_8U) in function 'cv::batchDistance'\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Paths to YOLOv3 files\n",
    "weights_path = r\"C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\yolov3\\yolov3.weights\"\n",
    "config_path = r\"C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\yolov3\\yolov3.cfg\"\n",
    "labels_path = r\"C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\yolov3\\coco.names\"\n",
    "\n",
    "# Load YOLO model\n",
    "net = cv2.dnn.readNetFromDarknet(config_path, weights_path)\n",
    "\n",
    "# Shi-Tomasi parameters\n",
    "maxCorners = 300\n",
    "qualityLevel = 0.01\n",
    "minDistance = 10\n",
    "\n",
    "# YOLO detection function with NMS\n",
    "def detect_objects(image, net):\n",
    "    (H, W) = image.shape[:2]\n",
    "    blob = cv2.dnn.blobFromImage(image, 1 / 255.0, (416, 416), swapRB=True, crop=False)\n",
    "    net.setInput(blob)\n",
    "    layer_names = net.getLayerNames()\n",
    "    output_layers = [layer_names[i - 1] for i in net.getUnconnectedOutLayers()]\n",
    "    layer_outputs = net.forward(output_layers)\n",
    "\n",
    "    boxes = []\n",
    "    confidences = []\n",
    "\n",
    "    for output in layer_outputs:\n",
    "        for detection in output:\n",
    "            scores = detection[5:]\n",
    "            class_id = int(np.argmax(scores))\n",
    "            confidence = scores[class_id]\n",
    "\n",
    "            if confidence > 0.5:  # Confidence threshold\n",
    "                box = detection[0:4] * np.array([W, H, W, H])\n",
    "                (centerX, centerY, width, height) = box.astype(\"int\")\n",
    "                x = int(centerX - (width / 2))\n",
    "                y = int(centerY - (height / 2))\n",
    "                boxes.append([x, y, int(width), int(height)])\n",
    "                confidences.append(float(confidence))\n",
    "\n",
    "    # Apply Non-Maximum Suppression (NMS)\n",
    "    indices = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)  # IoU and confidence thresholds\n",
    "    filtered_boxes = [boxes[i] for i in indices]\n",
    "\n",
    "    return filtered_boxes  # Return filtered bounding boxes\n",
    "\n",
    "# Filter corners by bounding boxes\n",
    "def filter_corners_by_boxes(corners, boxes):\n",
    "    filtered_corners = []\n",
    "    for corner in corners:\n",
    "        x, y = corner.ravel()\n",
    "        for (bx, by, bw, bh) in boxes:\n",
    "            if bx <= x <= bx + bw and by <= y <= by + bh:  # Check if corner is inside the box\n",
    "                filtered_corners.append(corner)\n",
    "                break\n",
    "    return np.array(filtered_corners)\n",
    "\n",
    "# Match features between two images for corresponding bounding boxes\n",
    "def match_features_in_boxes(image1, image2, corners1, corners2):\n",
    "    # Convert corners to keypoints\n",
    "    keypoints1 = [cv2.KeyPoint(c[0][0], c[0][1], 1) for c in corners1]\n",
    "    keypoints2 = [cv2.KeyPoint(c[0][0], c[0][1], 1) for c in corners2]\n",
    "\n",
    "    # Use ORB to compute descriptors\n",
    "    orb = cv2.ORB_create()\n",
    "    keypoints1, descriptors1 = orb.compute(image1, keypoints1)\n",
    "    keypoints2, descriptors2 = orb.compute(image2, keypoints2)\n",
    "\n",
    "    # Match features using Brute-Force matcher\n",
    "    bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "    matches = bf.match(descriptors1, descriptors2)\n",
    "\n",
    "    # Sort matches by distance\n",
    "    matches = sorted(matches, key=lambda x: x.distance)\n",
    "\n",
    "    return keypoints1, keypoints2, matches\n",
    "\n",
    "# Draw bounding boxes and matching lines\n",
    "def draw_boxes_and_matches(image1, image2, boxes1, boxes2, keypoints1, keypoints2, matches):\n",
    "    # Combine images side by side\n",
    "    combined_frame = cv2.hconcat([image1, image2])\n",
    "\n",
    "    # Draw bounding boxes\n",
    "    for (x, y, w, h) in boxes1:\n",
    "        cv2.rectangle(combined_frame, (x, y), (x + w, y + h), (0, 255, 0), 2)  # Green box for left\n",
    "    for (x, y, w, h) in boxes2:\n",
    "        cv2.rectangle(combined_frame, (x + image1.shape[1], y), (x + w + image1.shape[1], y + h), (0, 255, 0), 2)  # Adjusted for right image\n",
    "\n",
    "    # Draw matches\n",
    "    for match in matches:\n",
    "        pt1 = tuple(map(int, keypoints1[match.queryIdx].pt))  # Feature in left image\n",
    "        pt2 = tuple(map(int, keypoints2[match.trainIdx].pt))  # Feature in right image\n",
    "        pt2_shifted = (pt2[0] + image1.shape[1], pt2[1])  # Adjust for side-by-side alignment\n",
    "\n",
    "        # Draw the matching line\n",
    "        cv2.line(combined_frame, pt1, pt2_shifted, (255, 0, 0), 1)  # Blue line for matches\n",
    "\n",
    "    return combined_frame\n",
    "\n",
    "# Paths to the image folders\n",
    "left_folder = r\"C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Left5\"\n",
    "right_folder = r\"C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Right5\"\n",
    "\n",
    "# Get sorted list of images\n",
    "left_images = sorted([os.path.join(left_folder, f) for f in os.listdir(left_folder) if f.endswith(('.png', '.jpg', '.jpeg'))])\n",
    "right_images = sorted([os.path.join(right_folder, f) for f in os.listdir(right_folder) if f.endswith(('.png', '.jpg', '.jpeg'))])\n",
    "\n",
    "# Process images\n",
    "print(\"Processing images with YOLO, Shi-Tomasi, and Matching Lines...\")\n",
    "for idx, (left_image_path, right_image_path) in enumerate(zip(left_images, right_images)):\n",
    "    # Read images\n",
    "    left_image = cv2.imread(left_image_path)\n",
    "    right_image = cv2.imread(right_image_path)\n",
    "\n",
    "    if left_image is None or right_image is None:\n",
    "        print(f\"Error reading images: {left_image_path}, {right_image_path}\")\n",
    "        continue\n",
    "\n",
    "    # Convert images to grayscale\n",
    "    gray_left = cv2.cvtColor(left_image, cv2.COLOR_BGR2GRAY)\n",
    "    gray_right = cv2.cvtColor(right_image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Detect objects using YOLO and apply NMS\n",
    "    left_boxes = detect_objects(left_image, net)\n",
    "    right_boxes = detect_objects(right_image, net)\n",
    "\n",
    "    # Detect corners using Shi-Tomasi\n",
    "    corners_left = cv2.goodFeaturesToTrack(gray_left, maxCorners=maxCorners, qualityLevel=qualityLevel, minDistance=minDistance)\n",
    "    corners_right = cv2.goodFeaturesToTrack(gray_right, maxCorners=maxCorners, qualityLevel=qualityLevel, minDistance=minDistance)\n",
    "\n",
    "    # Filter corners inside bounding boxes\n",
    "    corners_left_filtered = filter_corners_by_boxes(corners_left, left_boxes) if corners_left is not None else []\n",
    "    corners_right_filtered = filter_corners_by_boxes(corners_right, right_boxes) if corners_right is not None else []\n",
    "\n",
    "    # Match features inside bounding boxes\n",
    "    keypoints_left, keypoints_right, matches = match_features_in_boxes(gray_left, gray_right, corners_left_filtered, corners_right_filtered)\n",
    "\n",
    "    # Draw bounding boxes and matches\n",
    "    matched_frame = draw_boxes_and_matches(left_image, right_image, left_boxes, right_boxes, keypoints_left, keypoints_right, matches)\n",
    "\n",
    "    # Display the matched frame\n",
    "    cv2.imshow(\"YOLO Bounding Boxes with Feature Matches\", matched_frame)\n",
    "\n",
    "    # Wait for 'q' to quit or 100ms per frame\n",
    "    if cv2.waitKey(100) & 0xFF == ord('q'):\n",
    "        print(\"Exiting display...\")\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# >>*YOLOv5* MATCHING IN BOUNDING BOX COMPLETED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\webin/.cache\\torch\\hub\\ultralytics_yolov5_master\n",
      "YOLOv5  2024-11-29 Python-3.12.4 torch-2.5.1+cpu CPU\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients, 16.4 GFLOPs\n",
      "Adding AutoShape... \n",
      "C:\\Users\\webin/.cache\\torch\\hub\\ultralytics_yolov5_master\\models\\common.py:892: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing images with YOLOv5, Shi-Tomasi, and Matching Lines...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\webin/.cache\\torch\\hub\\ultralytics_yolov5_master\\models\\common.py:892: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n",
      "C:\\Users\\webin/.cache\\torch\\hub\\ultralytics_yolov5_master\\models\\common.py:892: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n",
      "C:\\Users\\webin/.cache\\torch\\hub\\ultralytics_yolov5_master\\models\\common.py:892: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n",
      "C:\\Users\\webin/.cache\\torch\\hub\\ultralytics_yolov5_master\\models\\common.py:892: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n",
      "C:\\Users\\webin/.cache\\torch\\hub\\ultralytics_yolov5_master\\models\\common.py:892: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n",
      "C:\\Users\\webin/.cache\\torch\\hub\\ultralytics_yolov5_master\\models\\common.py:892: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n",
      "C:\\Users\\webin/.cache\\torch\\hub\\ultralytics_yolov5_master\\models\\common.py:892: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n",
      "C:\\Users\\webin/.cache\\torch\\hub\\ultralytics_yolov5_master\\models\\common.py:892: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n",
      "C:\\Users\\webin/.cache\\torch\\hub\\ultralytics_yolov5_master\\models\\common.py:892: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n",
      "C:\\Users\\webin/.cache\\torch\\hub\\ultralytics_yolov5_master\\models\\common.py:892: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n",
      "C:\\Users\\webin/.cache\\torch\\hub\\ultralytics_yolov5_master\\models\\common.py:892: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n",
      "C:\\Users\\webin/.cache\\torch\\hub\\ultralytics_yolov5_master\\models\\common.py:892: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n",
      "C:\\Users\\webin/.cache\\torch\\hub\\ultralytics_yolov5_master\\models\\common.py:892: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n",
      "C:\\Users\\webin/.cache\\torch\\hub\\ultralytics_yolov5_master\\models\\common.py:892: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n",
      "C:\\Users\\webin/.cache\\torch\\hub\\ultralytics_yolov5_master\\models\\common.py:892: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n",
      "C:\\Users\\webin/.cache\\torch\\hub\\ultralytics_yolov5_master\\models\\common.py:892: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n",
      "C:\\Users\\webin/.cache\\torch\\hub\\ultralytics_yolov5_master\\models\\common.py:892: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n",
      "C:\\Users\\webin/.cache\\torch\\hub\\ultralytics_yolov5_master\\models\\common.py:892: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n",
      "C:\\Users\\webin/.cache\\torch\\hub\\ultralytics_yolov5_master\\models\\common.py:892: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n",
      "C:\\Users\\webin/.cache\\torch\\hub\\ultralytics_yolov5_master\\models\\common.py:892: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n",
      "C:\\Users\\webin/.cache\\torch\\hub\\ultralytics_yolov5_master\\models\\common.py:892: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n",
      "C:\\Users\\webin/.cache\\torch\\hub\\ultralytics_yolov5_master\\models\\common.py:892: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n",
      "C:\\Users\\webin/.cache\\torch\\hub\\ultralytics_yolov5_master\\models\\common.py:892: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n",
      "C:\\Users\\webin/.cache\\torch\\hub\\ultralytics_yolov5_master\\models\\common.py:892: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n",
      "C:\\Users\\webin/.cache\\torch\\hub\\ultralytics_yolov5_master\\models\\common.py:892: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n",
      "C:\\Users\\webin/.cache\\torch\\hub\\ultralytics_yolov5_master\\models\\common.py:892: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n",
      "C:\\Users\\webin/.cache\\torch\\hub\\ultralytics_yolov5_master\\models\\common.py:892: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n",
      "C:\\Users\\webin/.cache\\torch\\hub\\ultralytics_yolov5_master\\models\\common.py:892: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n",
      "C:\\Users\\webin/.cache\\torch\\hub\\ultralytics_yolov5_master\\models\\common.py:892: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n",
      "C:\\Users\\webin/.cache\\torch\\hub\\ultralytics_yolov5_master\\models\\common.py:892: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n",
      "C:\\Users\\webin/.cache\\torch\\hub\\ultralytics_yolov5_master\\models\\common.py:892: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n",
      "C:\\Users\\webin/.cache\\torch\\hub\\ultralytics_yolov5_master\\models\\common.py:892: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n",
      "C:\\Users\\webin/.cache\\torch\\hub\\ultralytics_yolov5_master\\models\\common.py:892: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n",
      "C:\\Users\\webin/.cache\\torch\\hub\\ultralytics_yolov5_master\\models\\common.py:892: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n",
      "C:\\Users\\webin/.cache\\torch\\hub\\ultralytics_yolov5_master\\models\\common.py:892: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n",
      "C:\\Users\\webin/.cache\\torch\\hub\\ultralytics_yolov5_master\\models\\common.py:892: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n",
      "C:\\Users\\webin/.cache\\torch\\hub\\ultralytics_yolov5_master\\models\\common.py:892: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n",
      "C:\\Users\\webin/.cache\\torch\\hub\\ultralytics_yolov5_master\\models\\common.py:892: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n",
      "C:\\Users\\webin/.cache\\torch\\hub\\ultralytics_yolov5_master\\models\\common.py:892: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n",
      "C:\\Users\\webin/.cache\\torch\\hub\\ultralytics_yolov5_master\\models\\common.py:892: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n",
      "C:\\Users\\webin/.cache\\torch\\hub\\ultralytics_yolov5_master\\models\\common.py:892: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n",
      "C:\\Users\\webin/.cache\\torch\\hub\\ultralytics_yolov5_master\\models\\common.py:892: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n",
      "C:\\Users\\webin/.cache\\torch\\hub\\ultralytics_yolov5_master\\models\\common.py:892: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n",
      "C:\\Users\\webin/.cache\\torch\\hub\\ultralytics_yolov5_master\\models\\common.py:892: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n",
      "C:\\Users\\webin/.cache\\torch\\hub\\ultralytics_yolov5_master\\models\\common.py:892: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n",
      "C:\\Users\\webin/.cache\\torch\\hub\\ultralytics_yolov5_master\\models\\common.py:892: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n",
      "C:\\Users\\webin/.cache\\torch\\hub\\ultralytics_yolov5_master\\models\\common.py:892: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n",
      "C:\\Users\\webin/.cache\\torch\\hub\\ultralytics_yolov5_master\\models\\common.py:892: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n",
      "C:\\Users\\webin/.cache\\torch\\hub\\ultralytics_yolov5_master\\models\\common.py:892: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n",
      "C:\\Users\\webin/.cache\\torch\\hub\\ultralytics_yolov5_master\\models\\common.py:892: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n",
      "C:\\Users\\webin/.cache\\torch\\hub\\ultralytics_yolov5_master\\models\\common.py:892: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n",
      "C:\\Users\\webin/.cache\\torch\\hub\\ultralytics_yolov5_master\\models\\common.py:892: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n",
      "C:\\Users\\webin/.cache\\torch\\hub\\ultralytics_yolov5_master\\models\\common.py:892: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n",
      "C:\\Users\\webin/.cache\\torch\\hub\\ultralytics_yolov5_master\\models\\common.py:892: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n",
      "C:\\Users\\webin/.cache\\torch\\hub\\ultralytics_yolov5_master\\models\\common.py:892: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n",
      "C:\\Users\\webin/.cache\\torch\\hub\\ultralytics_yolov5_master\\models\\common.py:892: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n",
      "C:\\Users\\webin/.cache\\torch\\hub\\ultralytics_yolov5_master\\models\\common.py:892: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n",
      "C:\\Users\\webin/.cache\\torch\\hub\\ultralytics_yolov5_master\\models\\common.py:892: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n",
      "C:\\Users\\webin/.cache\\torch\\hub\\ultralytics_yolov5_master\\models\\common.py:892: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n",
      "C:\\Users\\webin/.cache\\torch\\hub\\ultralytics_yolov5_master\\models\\common.py:892: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n",
      "C:\\Users\\webin/.cache\\torch\\hub\\ultralytics_yolov5_master\\models\\common.py:892: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n",
      "C:\\Users\\webin/.cache\\torch\\hub\\ultralytics_yolov5_master\\models\\common.py:892: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n",
      "C:\\Users\\webin/.cache\\torch\\hub\\ultralytics_yolov5_master\\models\\common.py:892: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n",
      "C:\\Users\\webin/.cache\\torch\\hub\\ultralytics_yolov5_master\\models\\common.py:892: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n",
      "C:\\Users\\webin/.cache\\torch\\hub\\ultralytics_yolov5_master\\models\\common.py:892: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n",
      "C:\\Users\\webin/.cache\\torch\\hub\\ultralytics_yolov5_master\\models\\common.py:892: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n",
      "C:\\Users\\webin/.cache\\torch\\hub\\ultralytics_yolov5_master\\models\\common.py:892: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n",
      "C:\\Users\\webin/.cache\\torch\\hub\\ultralytics_yolov5_master\\models\\common.py:892: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n",
      "C:\\Users\\webin/.cache\\torch\\hub\\ultralytics_yolov5_master\\models\\common.py:892: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n",
      "C:\\Users\\webin/.cache\\torch\\hub\\ultralytics_yolov5_master\\models\\common.py:892: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n",
      "C:\\Users\\webin/.cache\\torch\\hub\\ultralytics_yolov5_master\\models\\common.py:892: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n",
      "C:\\Users\\webin/.cache\\torch\\hub\\ultralytics_yolov5_master\\models\\common.py:892: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n",
      "C:\\Users\\webin/.cache\\torch\\hub\\ultralytics_yolov5_master\\models\\common.py:892: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n",
      "C:\\Users\\webin/.cache\\torch\\hub\\ultralytics_yolov5_master\\models\\common.py:892: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n",
      "C:\\Users\\webin/.cache\\torch\\hub\\ultralytics_yolov5_master\\models\\common.py:892: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n",
      "C:\\Users\\webin/.cache\\torch\\hub\\ultralytics_yolov5_master\\models\\common.py:892: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n",
      "C:\\Users\\webin/.cache\\torch\\hub\\ultralytics_yolov5_master\\models\\common.py:892: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n",
      "C:\\Users\\webin/.cache\\torch\\hub\\ultralytics_yolov5_master\\models\\common.py:892: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n",
      "C:\\Users\\webin/.cache\\torch\\hub\\ultralytics_yolov5_master\\models\\common.py:892: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n",
      "C:\\Users\\webin/.cache\\torch\\hub\\ultralytics_yolov5_master\\models\\common.py:892: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n",
      "C:\\Users\\webin/.cache\\torch\\hub\\ultralytics_yolov5_master\\models\\common.py:892: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n",
      "C:\\Users\\webin/.cache\\torch\\hub\\ultralytics_yolov5_master\\models\\common.py:892: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n",
      "C:\\Users\\webin/.cache\\torch\\hub\\ultralytics_yolov5_master\\models\\common.py:892: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n",
      "C:\\Users\\webin/.cache\\torch\\hub\\ultralytics_yolov5_master\\models\\common.py:892: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n",
      "C:\\Users\\webin/.cache\\torch\\hub\\ultralytics_yolov5_master\\models\\common.py:892: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.10.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\core\\src\\batch_distance.cpp:275: error: (-215:Assertion failed) type == src2.type() && src1.cols == src2.cols && (type == CV_32F || type == CV_8U) in function 'cv::batchDistance'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 117\u001b[0m\n\u001b[0;32m    114\u001b[0m corners_right_filtered \u001b[38;5;241m=\u001b[39m filter_corners_by_boxes(corners_right, right_boxes) \u001b[38;5;28;01mif\u001b[39;00m corners_right \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m []\n\u001b[0;32m    116\u001b[0m \u001b[38;5;66;03m# Match features inside bounding boxes\u001b[39;00m\n\u001b[1;32m--> 117\u001b[0m keypoints_left, keypoints_right, matches \u001b[38;5;241m=\u001b[39m match_features_in_boxes(gray_left, gray_right, corners_left_filtered, corners_right_filtered)\n\u001b[0;32m    119\u001b[0m \u001b[38;5;66;03m# Draw bounding boxes and matches\u001b[39;00m\n\u001b[0;32m    120\u001b[0m matched_frame \u001b[38;5;241m=\u001b[39m draw_boxes_and_matches(left_image, right_image, left_boxes, right_boxes, keypoints_left, keypoints_right, matches)\n",
      "Cell \u001b[1;32mIn[10], line 52\u001b[0m, in \u001b[0;36mmatch_features_in_boxes\u001b[1;34m(image1, image2, corners1, corners2)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# Match features using Brute-Force matcher\u001b[39;00m\n\u001b[0;32m     51\u001b[0m bf \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mBFMatcher(cv2\u001b[38;5;241m.\u001b[39mNORM_HAMMING, crossCheck\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 52\u001b[0m matches \u001b[38;5;241m=\u001b[39m bf\u001b[38;5;241m.\u001b[39mmatch(descriptors1, descriptors2)\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m# Sort matches by distance\u001b[39;00m\n\u001b[0;32m     55\u001b[0m matches \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(matches, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: x\u001b[38;5;241m.\u001b[39mdistance)\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.10.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\core\\src\\batch_distance.cpp:275: error: (-215:Assertion failed) type == src2.type() && src1.cols == src2.cols && (type == CV_32F || type == CV_8U) in function 'cv::batchDistance'\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Load the YOLOv5 model (use \"yolov5s\" for a small and fast model)\n",
    "model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)\n",
    "\n",
    "# Shi-Tomasi parameters\n",
    "maxCorners = 300\n",
    "qualityLevel = 0.01\n",
    "minDistance = 10\n",
    "\n",
    "# YOLO detection function\n",
    "def detect_objects(image, model):\n",
    "    # Perform YOLOv5 inference\n",
    "    results = model(image)\n",
    "    detections = results.xyxy[0].numpy()  # Extract detection results as NumPy array\n",
    "\n",
    "    boxes = []\n",
    "    for detection in detections:\n",
    "        x_min, y_min, x_max, y_max, confidence, class_id = detection\n",
    "        if confidence > 0.5:  # Confidence threshold\n",
    "            boxes.append([int(x_min), int(y_min), int(x_max - x_min), int(y_max - y_min)])\n",
    "\n",
    "    return boxes  # Return bounding boxes\n",
    "\n",
    "# Filter corners by bounding boxes\n",
    "def filter_corners_by_boxes(corners, boxes):\n",
    "    filtered_corners = []\n",
    "    for corner in corners:\n",
    "        x, y = corner.ravel()\n",
    "        for (bx, by, bw, bh) in boxes:\n",
    "            if bx <= x <= bx + bw and by <= y <= by + bh:  # Check if corner is inside the box\n",
    "                filtered_corners.append(corner)\n",
    "                break\n",
    "    return np.array(filtered_corners)\n",
    "\n",
    "# Match features between two images for corresponding bounding boxes\n",
    "def match_features_in_boxes(image1, image2, corners1, corners2):\n",
    "    # Convert corners to keypoints\n",
    "    keypoints1 = [cv2.KeyPoint(c[0][0], c[0][1], 1) for c in corners1]\n",
    "    keypoints2 = [cv2.KeyPoint(c[0][0], c[0][1], 1) for c in corners2]\n",
    "\n",
    "    # Use ORB to compute descriptors\n",
    "    orb = cv2.ORB_create()\n",
    "    keypoints1, descriptors1 = orb.compute(image1, keypoints1)\n",
    "    keypoints2, descriptors2 = orb.compute(image2, keypoints2)\n",
    "\n",
    "    # Match features using Brute-Force matcher\n",
    "    bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "    matches = bf.match(descriptors1, descriptors2)\n",
    "\n",
    "    # Sort matches by distance\n",
    "    matches = sorted(matches, key=lambda x: x.distance)\n",
    "\n",
    "    return keypoints1, keypoints2, matches\n",
    "\n",
    "# Draw bounding boxes and matching lines\n",
    "def draw_boxes_and_matches(image1, image2, boxes1, boxes2, keypoints1, keypoints2, matches):\n",
    "    # Combine images side by side\n",
    "    combined_frame = cv2.hconcat([image1, image2])\n",
    "\n",
    "    # Draw bounding boxes\n",
    "    for (x, y, w, h) in boxes1:\n",
    "        cv2.rectangle(combined_frame, (x, y), (x + w, y + h), (0, 255, 0), 2)  # Green box for left\n",
    "    for (x, y, w, h) in boxes2:\n",
    "        cv2.rectangle(combined_frame, (x + image1.shape[1], y), (x + w + image1.shape[1], y + h), (0, 255, 0), 2)  # Adjusted for right image\n",
    "\n",
    "    # Draw matches\n",
    "    for match in matches:\n",
    "        pt1 = tuple(map(int, keypoints1[match.queryIdx].pt))  # Feature in left image\n",
    "        pt2 = tuple(map(int, keypoints2[match.trainIdx].pt))  # Feature in right image\n",
    "        pt2_shifted = (pt2[0] + image1.shape[1], pt2[1])  # Adjust for side-by-side alignment\n",
    "\n",
    "        # Draw the matching line\n",
    "        cv2.line(combined_frame, pt1, pt2_shifted, (255, 0, 0), 1)  # Blue line for matches\n",
    "\n",
    "    return combined_frame\n",
    "\n",
    "# Paths to the image folders\n",
    "left_folder = r\"C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Left5\"\n",
    "right_folder = r\"C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Right5\"\n",
    "\n",
    "# Get sorted list of images\n",
    "left_images = sorted([os.path.join(left_folder, f) for f in os.listdir(left_folder) if f.endswith(('.png', '.jpg', '.jpeg'))])\n",
    "right_images = sorted([os.path.join(right_folder, f) for f in os.listdir(right_folder) if f.endswith(('.png', '.jpg', '.jpeg'))])\n",
    "\n",
    "# Process images\n",
    "print(\"Processing images with YOLOv5, Shi-Tomasi, and Matching Lines...\")\n",
    "for idx, (left_image_path, right_image_path) in enumerate(zip(left_images, right_images)):\n",
    "    # Read images\n",
    "    left_image = cv2.imread(left_image_path)\n",
    "    right_image = cv2.imread(right_image_path)\n",
    "\n",
    "    if left_image is None or right_image is None:\n",
    "        print(f\"Error reading images: {left_image_path}, {right_image_path}\")\n",
    "        continue\n",
    "\n",
    "    # Convert images to grayscale\n",
    "    gray_left = cv2.cvtColor(left_image, cv2.COLOR_BGR2GRAY)\n",
    "    gray_right = cv2.cvtColor(right_image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Detect objects using YOLOv5\n",
    "    left_boxes = detect_objects(left_image, model)\n",
    "    right_boxes = detect_objects(right_image, model)\n",
    "\n",
    "    # Detect corners using Shi-Tomasi\n",
    "    corners_left = cv2.goodFeaturesToTrack(gray_left, maxCorners=maxCorners, qualityLevel=qualityLevel, minDistance=minDistance)\n",
    "    corners_right = cv2.goodFeaturesToTrack(gray_right, maxCorners=maxCorners, qualityLevel=qualityLevel, minDistance=minDistance)\n",
    "\n",
    "    # Filter corners inside bounding boxes\n",
    "    corners_left_filtered = filter_corners_by_boxes(corners_left, left_boxes) if corners_left is not None else []\n",
    "    corners_right_filtered = filter_corners_by_boxes(corners_right, right_boxes) if corners_right is not None else []\n",
    "\n",
    "    # Match features inside bounding boxes\n",
    "    keypoints_left, keypoints_right, matches = match_features_in_boxes(gray_left, gray_right, corners_left_filtered, corners_right_filtered)\n",
    "\n",
    "    # Draw bounding boxes and matches\n",
    "    matched_frame = draw_boxes_and_matches(left_image, right_image, left_boxes, right_boxes, keypoints_left, keypoints_right, matches)\n",
    "\n",
    "    # Display the matched frame\n",
    "    cv2.imshow(\"YOLOv5 Bounding Boxes with Feature Matches\", matched_frame)\n",
    "\n",
    "    # Wait for 'q' to quit or 100ms per frame\n",
    "    if cv2.waitKey(100) & 0xFF == ord('q'):\n",
    "        print(\"Exiting display...\")\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\webin/.cache\\torch\\hub\\ultralytics_yolov5_master\n",
      "YOLOv5  2024-11-29 Python-3.12.4 torch-2.5.1+cpu CPU\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients, 16.4 GFLOPs\n",
      "Adding AutoShape... \n",
      "C:\\Users\\webin/.cache\\torch\\hub\\ultralytics_yolov5_master\\models\\common.py:892: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing images with YOLOv5, Shi-Tomasi, and Matching Lines...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\webin/.cache\\torch\\hub\\ultralytics_yolov5_master\\models\\common.py:892: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No descriptors found in one of the images.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\webin/.cache\\torch\\hub\\ultralytics_yolov5_master\\models\\common.py:892: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n",
      "C:\\Users\\webin/.cache\\torch\\hub\\ultralytics_yolov5_master\\models\\common.py:892: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No descriptors found in one of the images.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\webin/.cache\\torch\\hub\\ultralytics_yolov5_master\\models\\common.py:892: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n",
      "C:\\Users\\webin/.cache\\torch\\hub\\ultralytics_yolov5_master\\models\\common.py:892: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.10.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\core\\src\\batch_distance.cpp:282: error: (-215:Assertion failed) (type == CV_8U && dtype == CV_32S) || dtype == CV_32F in function 'cv::batchDistance'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 133\u001b[0m\n\u001b[0;32m    130\u001b[0m corners_right_filtered \u001b[38;5;241m=\u001b[39m filter_corners_by_boxes(corners_right, right_boxes) \u001b[38;5;28;01mif\u001b[39;00m corners_right \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m []\n\u001b[0;32m    132\u001b[0m \u001b[38;5;66;03m# Match features inside bounding boxes\u001b[39;00m\n\u001b[1;32m--> 133\u001b[0m keypoints_left, keypoints_right, matches \u001b[38;5;241m=\u001b[39m match_features_in_boxes(gray_left, gray_right, corners_left_filtered, corners_right_filtered)\n\u001b[0;32m    135\u001b[0m \u001b[38;5;66;03m# Draw bounding boxes and matches\u001b[39;00m\n\u001b[0;32m    136\u001b[0m matched_frame \u001b[38;5;241m=\u001b[39m draw_boxes_and_matches(left_image, right_image, left_boxes, right_boxes, keypoints_left, keypoints_right, matches)\n",
      "Cell \u001b[1;32mIn[1], line 67\u001b[0m, in \u001b[0;36mmatch_features_in_boxes\u001b[1;34m(image1, image2, corners1, corners2)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;66;03m# Match features using Brute-Force matcher\u001b[39;00m\n\u001b[0;32m     66\u001b[0m bf \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mBFMatcher(cv2\u001b[38;5;241m.\u001b[39mNORM_HAMMING, crossCheck\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 67\u001b[0m matches \u001b[38;5;241m=\u001b[39m bf\u001b[38;5;241m.\u001b[39mmatch(descriptors1, descriptors2)\n\u001b[0;32m     69\u001b[0m \u001b[38;5;66;03m# Sort matches by distance\u001b[39;00m\n\u001b[0;32m     70\u001b[0m matches \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(matches, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: x\u001b[38;5;241m.\u001b[39mdistance)\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.10.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\core\\src\\batch_distance.cpp:282: error: (-215:Assertion failed) (type == CV_8U && dtype == CV_32S) || dtype == CV_32F in function 'cv::batchDistance'\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Load the YOLOv5 model (use \"yolov5s\" for a small and fast model)\n",
    "model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)\n",
    "\n",
    "# Shi-Tomasi parameters\n",
    "maxCorners = 300\n",
    "qualityLevel = 0.01\n",
    "minDistance = 10\n",
    "\n",
    "# YOLO detection function\n",
    "def detect_objects(image, model):\n",
    "    # Perform YOLOv5 inference\n",
    "    results = model(image)\n",
    "    detections = results.xyxy[0].numpy()  # Extract detection results as NumPy array\n",
    "\n",
    "    boxes = []\n",
    "    for detection in detections:\n",
    "        x_min, y_min, x_max, y_max, confidence, class_id = detection\n",
    "        if confidence > 0.5:  # Confidence threshold\n",
    "            boxes.append([int(x_min), int(y_min), int(x_max - x_min), int(y_max - y_min)])\n",
    "\n",
    "    return boxes  # Return bounding boxes\n",
    "\n",
    "# Filter corners by bounding boxes\n",
    "def filter_corners_by_boxes(corners, boxes):\n",
    "    filtered_corners = []\n",
    "    for corner in corners:\n",
    "        x, y = corner.ravel()\n",
    "        for (bx, by, bw, bh) in boxes:\n",
    "            if bx <= x <= bx + bw and by <= y <= by + bh:  # Check if corner is inside the box\n",
    "                filtered_corners.append(corner)\n",
    "                break\n",
    "    return np.array(filtered_corners)\n",
    "\n",
    "# Match features between two images for corresponding bounding boxes\n",
    "def match_features_in_boxes(image1, image2, corners1, corners2):\n",
    "    # Convert corners to keypoints\n",
    "    keypoints1 = [cv2.KeyPoint(c[0][0], c[0][1], 1) for c in corners1]\n",
    "    keypoints2 = [cv2.KeyPoint(c[0][0], c[0][1], 1) for c in corners2]\n",
    "\n",
    "    # Use ORB to compute descriptors\n",
    "    orb = cv2.ORB_create()\n",
    "    keypoints1, descriptors1 = orb.compute(image1, keypoints1)\n",
    "    keypoints2, descriptors2 = orb.compute(image2, keypoints2)\n",
    "\n",
    "    # Handle cases where descriptors are None or empty\n",
    "    if descriptors1 is None or descriptors2 is None:\n",
    "        print(\"No descriptors found in one of the images.\")\n",
    "        return keypoints1, keypoints2, []  # Return empty matches\n",
    "\n",
    "    if len(descriptors1) == 0 or len(descriptors2) == 0:\n",
    "        print(\"Empty descriptors detected.\")\n",
    "        return keypoints1, keypoints2, []  # Return empty matches\n",
    "\n",
    "    # Ensure descriptors are of type CV_32F\n",
    "    if descriptors1.dtype != np.float32:\n",
    "        descriptors1 = np.float32(descriptors1)\n",
    "    if descriptors2.dtype != np.float32:\n",
    "        descriptors2 = np.float32(descriptors2)\n",
    "\n",
    "    # Match features using Brute-Force matcher\n",
    "    bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "    matches = bf.match(descriptors1, descriptors2)\n",
    "\n",
    "    # Sort matches by distance\n",
    "    matches = sorted(matches, key=lambda x: x.distance)\n",
    "\n",
    "    return keypoints1, keypoints2, matches\n",
    "\n",
    "\n",
    "# Draw bounding boxes and matching lines\n",
    "def draw_boxes_and_matches(image1, image2, boxes1, boxes2, keypoints1, keypoints2, matches):\n",
    "    # Combine images side by side\n",
    "    combined_frame = cv2.hconcat([image1, image2])\n",
    "\n",
    "    # Draw bounding boxes\n",
    "    for (x, y, w, h) in boxes1:\n",
    "        cv2.rectangle(combined_frame, (x, y), (x + w, y + h), (0, 255, 0), 2)  # Green box for left\n",
    "    for (x, y, w, h) in boxes2:\n",
    "        cv2.rectangle(combined_frame, (x + image1.shape[1], y), (x + w + image1.shape[1], y + h), (0, 255, 0), 2)  # Adjusted for right image\n",
    "\n",
    "    # Draw matches\n",
    "    for match in matches:\n",
    "        pt1 = tuple(map(int, keypoints1[match.queryIdx].pt))  # Feature in left image\n",
    "        pt2 = tuple(map(int, keypoints2[match.trainIdx].pt))  # Feature in right image\n",
    "        pt2_shifted = (pt2[0] + image1.shape[1], pt2[1])  # Adjust for side-by-side alignment\n",
    "\n",
    "        # Draw the matching line\n",
    "        cv2.line(combined_frame, pt1, pt2_shifted, (255, 0, 0), 1)  # Blue line for matches\n",
    "\n",
    "    return combined_frame\n",
    "\n",
    "# Paths to the image folders\n",
    "left_folder = r\"C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Left5\"\n",
    "right_folder = r\"C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Right5\"\n",
    "\n",
    "# Get sorted list of images\n",
    "left_images = sorted([os.path.join(left_folder, f) for f in os.listdir(left_folder) if f.endswith(('.png', '.jpg', '.jpeg'))])\n",
    "right_images = sorted([os.path.join(right_folder, f) for f in os.listdir(right_folder) if f.endswith(('.png', '.jpg', '.jpeg'))])\n",
    "\n",
    "# Process images\n",
    "print(\"Processing images with YOLOv5, Shi-Tomasi, and Matching Lines...\")\n",
    "for idx, (left_image_path, right_image_path) in enumerate(zip(left_images, right_images)):\n",
    "    # Read images\n",
    "    left_image = cv2.imread(left_image_path)\n",
    "    right_image = cv2.imread(right_image_path)\n",
    "\n",
    "    if left_image is None or right_image is None:\n",
    "        print(f\"Error reading images: {left_image_path}, {right_image_path}\")\n",
    "        continue\n",
    "\n",
    "    # Convert images to grayscale\n",
    "    gray_left = cv2.cvtColor(left_image, cv2.COLOR_BGR2GRAY)\n",
    "    gray_right = cv2.cvtColor(right_image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Detect objects using YOLOv5\n",
    "    left_boxes = detect_objects(left_image, model)\n",
    "    right_boxes = detect_objects(right_image, model)\n",
    "\n",
    "    # Detect corners using Shi-Tomasi\n",
    "    corners_left = cv2.goodFeaturesToTrack(gray_left, maxCorners=maxCorners, qualityLevel=qualityLevel, minDistance=minDistance)\n",
    "    corners_right = cv2.goodFeaturesToTrack(gray_right, maxCorners=maxCorners, qualityLevel=qualityLevel, minDistance=minDistance)\n",
    "\n",
    "    # Filter corners inside bounding boxes\n",
    "    corners_left_filtered = filter_corners_by_boxes(corners_left, left_boxes) if corners_left is not None else []\n",
    "    corners_right_filtered = filter_corners_by_boxes(corners_right, right_boxes) if corners_right is not None else []\n",
    "\n",
    "    # Match features inside bounding boxes\n",
    "    keypoints_left, keypoints_right, matches = match_features_in_boxes(gray_left, gray_right, corners_left_filtered, corners_right_filtered)\n",
    "\n",
    "    # Draw bounding boxes and matches\n",
    "    matched_frame = draw_boxes_and_matches(left_image, right_image, left_boxes, right_boxes, keypoints_left, keypoints_right, matches)\n",
    "\n",
    "    # Display the matched frame\n",
    "    cv2.imshow(\"YOLOv5 Bounding Boxes with Feature Matches\", matched_frame)\n",
    "\n",
    "    # Wait for 'q' to quit or 100ms per frame\n",
    "    if cv2.waitKey(100) & 0xFF == ord('q'):\n",
    "        print(\"Exiting display...\")\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply Harris Corner Detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing images with Harris Corner Detector...\n",
      "Exiting display...\n",
      "Processed all images. No video saved.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Paths to the image folders\n",
    "left_folder = r\"C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Left5\"\n",
    "right_folder = r\"C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Right5\"\n",
    "\n",
    "# Output video path\n",
    "output_video_path = r\"C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\harris_corners_video.avi\"\n",
    "\n",
    "# User choice: Save video? (True/False) ####\n",
    "save_video = False  # Change to True if you want to save the processed video\n",
    "\n",
    "# Adjust Feature Extractor Parameter\n",
    "blockSize_my = 3\n",
    "ksize_my = 3\n",
    "k_my = 0.10\n",
    "corner_ratio = 0.005\n",
    "\n",
    "# Get sorted list of images\n",
    "left_images = sorted([os.path.join(left_folder, f) for f in os.listdir(left_folder) if f.endswith(('.png', '.jpg', '.jpeg'))])\n",
    "right_images = sorted([os.path.join(right_folder, f) for f in os.listdir(right_folder) if f.endswith(('.png', '.jpg', '.jpeg'))])\n",
    "\n",
    "# Ensure both folders have the same number of images\n",
    "if len(left_images) != len(right_images):\n",
    "    print(\"Error: The number of images in the left and right folders does not match.\")\n",
    "    exit()\n",
    "\n",
    "# Initialize video writer if saving video\n",
    "out = None\n",
    "if save_video:\n",
    "    frame_size = (1280, 480)  # Combined frame size (640x480 for each image)\n",
    "    fps = 10  # Frames per second\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "    out = cv2.VideoWriter(output_video_path, fourcc, fps, frame_size)\n",
    "\n",
    "# Process images\n",
    "print(\"Processing images with Harris Corner Detector...\")\n",
    "for left_image_path, right_image_path in zip(left_images, right_images):\n",
    "    # Read images\n",
    "    left_image = cv2.imread(left_image_path)\n",
    "    right_image = cv2.imread(right_image_path)\n",
    "\n",
    "    if left_image is None or right_image is None:\n",
    "        print(f\"Error reading images: {left_image_path}, {right_image_path}\")\n",
    "        continue\n",
    "\n",
    "    # Resize images to ensure the same dimensions\n",
    "    left_image = cv2.resize(left_image, (640, 480))\n",
    "    right_image = cv2.resize(right_image, (640, 480))\n",
    "\n",
    "    # Convert images to grayscale\n",
    "    gray_left = cv2.cvtColor(left_image, cv2.COLOR_BGR2GRAY)\n",
    "    gray_right = cv2.cvtColor(right_image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Apply Harris Corner Detection\n",
    "    corners_left = cv2.cornerHarris(np.float32(gray_left), blockSize=blockSize_my, ksize=ksize_my, k=k_my)\n",
    "    corners_right = cv2.cornerHarris(np.float32(gray_right), blockSize=blockSize_my, ksize=ksize_my, k=k_my)\n",
    "\n",
    "    # Dilate corners to enhance visualization\n",
    "    corners_left = cv2.dilate(corners_left, None)\n",
    "    corners_right = cv2.dilate(corners_right, None)\n",
    "\n",
    "    # Mark corners in the images\n",
    "    left_image[corners_left   > corner_ratio * corners_left.max()]  = [0, 0, 255]  # Red color for corners\n",
    "    right_image[corners_right > corner_ratio * corners_right.max()] = [0, 0, 255]\n",
    "\n",
    "    # Combine left and right images side by side\n",
    "    combined_frame = cv2.hconcat([left_image, right_image])\n",
    "\n",
    "    # Display the combined frame\n",
    "    cv2.imshow(\"Harris Corners Detection\", combined_frame)\n",
    "\n",
    "    # Save the combined frame to video if enabled\n",
    "    if save_video and out is not None:\n",
    "        #out.write(combined_frame)\n",
    "        pass\n",
    "\n",
    "    # Wait for 'q' to quit or 100ms per frame\n",
    "    if cv2.waitKey(100) & 0xFF == ord('q'):\n",
    "        print(\"Exiting display...\")\n",
    "        break\n",
    "\n",
    "# Release resources\n",
    "if out:\n",
    "    out.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "if save_video:\n",
    "    print(f\"Video saved at {output_video_path}\")\n",
    "else:\n",
    "    print(\"Processed all images. No video saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply Shi-Tomasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing images with Shi-Tomasi Corner Detection...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\webin\\AppData\\Local\\Temp\\ipykernel_20408\\815133768.py:61: DeprecationWarning: `np.int0` is a deprecated alias for `np.intp`.  (Deprecated NumPy 1.24)\n",
      "  x, y = np.int0(corner.ravel())\n",
      "C:\\Users\\webin\\AppData\\Local\\Temp\\ipykernel_20408\\815133768.py:65: DeprecationWarning: `np.int0` is a deprecated alias for `np.intp`.  (Deprecated NumPy 1.24)\n",
      "  x, y = np.int0(corner.ravel())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed all images. No images saved.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Paths to the image folders\n",
    "left_folder = r\"C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Left5\"\n",
    "right_folder = r\"C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Right5\"\n",
    "\n",
    "# Output folders for processed images\n",
    "output_left_folder = r\"C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Shi_Tomasi_Left5\"\n",
    "output_right_folder = r\"C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Shi_Tomasi_Right5\"\n",
    "\n",
    "# User choice: Save images? (True/False)\n",
    "save_images = False  # Set to True if you want to save processed frames as images\n",
    "\n",
    "# Create output directories if saving is enabled\n",
    "if save_images:\n",
    "    os.makedirs(output_left_folder, exist_ok=True)\n",
    "    os.makedirs(output_right_folder, exist_ok=True)\n",
    "\n",
    "# Shi-Tomasi parameters\n",
    "maxCorners_my = 500\n",
    "qualityLevel_my = 0.01\n",
    "minDistance_my = 10\n",
    "\n",
    "# Get sorted list of images\n",
    "left_images = sorted([os.path.join(left_folder, f) for f in os.listdir(left_folder) if f.endswith(('.png', '.jpg', '.jpeg'))])\n",
    "right_images = sorted([os.path.join(right_folder, f) for f in os.listdir(right_folder) if f.endswith(('.png', '.jpg', '.jpeg'))])\n",
    "\n",
    "# Ensure both folders have the same number of images\n",
    "if len(left_images) != len(right_images):\n",
    "    print(\"Error: The number of images in the left and right folders does not match.\")\n",
    "    exit()\n",
    "\n",
    "# Process images\n",
    "print(\"Processing images with Shi-Tomasi Corner Detection...\")\n",
    "for idx, (left_image_path, right_image_path) in enumerate(zip(left_images, right_images)):\n",
    "    # Read images\n",
    "    left_image = cv2.imread(left_image_path)\n",
    "    right_image = cv2.imread(right_image_path)\n",
    "\n",
    "    if left_image is None or right_image is None:\n",
    "        print(f\"Error reading images: {left_image_path}, {right_image_path}\")\n",
    "        continue\n",
    "\n",
    "    # Resize images to ensure the same dimensions\n",
    "    left_image = cv2.resize(left_image, (640, 480))\n",
    "    right_image = cv2.resize(right_image, (640, 480))\n",
    "\n",
    "    # Convert images to grayscale\n",
    "    gray_left = cv2.cvtColor(left_image, cv2.COLOR_BGR2GRAY)\n",
    "    gray_right = cv2.cvtColor(right_image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Apply Shi-Tomasi Corner Detection\n",
    "    corners_left = cv2.goodFeaturesToTrack(gray_left, maxCorners=maxCorners_my, qualityLevel=qualityLevel_my, minDistance=minDistance_my)\n",
    "    corners_right = cv2.goodFeaturesToTrack(gray_right, maxCorners=maxCorners_my, qualityLevel=qualityLevel_my, minDistance=minDistance_my)\n",
    "\n",
    "    # Draw smaller corners on the images\n",
    "    if corners_left is not None:\n",
    "        for corner in corners_left:\n",
    "            x, y = np.int0(corner.ravel())\n",
    "            cv2.circle(left_image, (x, y), 2, (0, 255, 0), -1)  # Smaller green circles for corners\n",
    "    if corners_right is not None:\n",
    "        for corner in corners_right:\n",
    "            x, y = np.int0(corner.ravel())\n",
    "            cv2.circle(right_image, (x, y), 2, (0, 255, 0), -1)  # Smaller green circles for corners\n",
    "\n",
    "    # Save the processed frames as images if enabled\n",
    "    if save_images:\n",
    "        output_left_image_path = os.path.join(output_left_folder, f\"left_{idx:04d}.png\")\n",
    "        output_right_image_path = os.path.join(output_right_folder, f\"right_{idx:04d}.png\")\n",
    "        cv2.imwrite(output_left_image_path, left_image)\n",
    "        cv2.imwrite(output_right_image_path, right_image)\n",
    "        print(f\"Saved: {output_left_image_path}, {output_right_image_path}\")\n",
    "\n",
    "    # Combine left and right images side by side for display\n",
    "    combined_frame = cv2.hconcat([left_image, right_image])\n",
    "\n",
    "    # Display the combined frame\n",
    "    cv2.imshow(\"Shi-Tomasi Corners Detection\", combined_frame)\n",
    "\n",
    "    # Wait for 'q' to quit or 100ms per frame\n",
    "    if cv2.waitKey(100) & 0xFF == ord('q'):\n",
    "        print(\"Exiting display...\")\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "if save_images:\n",
    "    print(f\"Processed images saved in:\\nLeft folder: {output_left_folder}\\nRight folder: {output_right_folder}\")\n",
    "else:\n",
    "    print(\"Processed all images. No images saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing feature matching...\n",
      "Feature matching completed!\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Paths to the image folders\n",
    "left_folder = r\"C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Left5\"\n",
    "right_folder = r\"C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Right5\"\n",
    "\n",
    "# Output folders for processed matches\n",
    "output_folder = r\"C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Feature_Matching_5\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# User choice: Save matches? (True/False)\n",
    "save_matches = False  # Set to True if you want to save matched images\n",
    "\n",
    "# Shi-Tomasi parameters\n",
    "maxCorners_my = 500\n",
    "qualityLevel_my = 0.01\n",
    "minDistance_my = 10\n",
    "\n",
    "# ORB descriptor extractor\n",
    "orb = cv2.ORB_create()\n",
    "\n",
    "# Get sorted list of images\n",
    "left_images = sorted([os.path.join(left_folder, f) for f in os.listdir(left_folder) if f.endswith(('.png', '.jpg', '.jpeg'))])\n",
    "right_images = sorted([os.path.join(right_folder, f) for f in os.listdir(right_folder) if f.endswith(('.png', '.jpg', '.jpeg'))])\n",
    "\n",
    "# Ensure both folders have the same number of images\n",
    "if len(left_images) != len(right_images):\n",
    "    print(\"Error: The number of images in the left and right folders does not match.\")\n",
    "    exit()\n",
    "\n",
    "# Process images\n",
    "print(\"Performing feature matching...\")\n",
    "for idx, (left_image_path, right_image_path) in enumerate(zip(left_images, right_images)):\n",
    "    # Read images\n",
    "    left_image = cv2.imread(left_image_path)\n",
    "    right_image = cv2.imread(right_image_path)\n",
    "\n",
    "    if left_image is None or right_image is None:\n",
    "        print(f\"Error reading images: {left_image_path}, {right_image_path}\")\n",
    "        continue\n",
    "\n",
    "    # Resize images to ensure the same dimensions\n",
    "    left_image = cv2.resize(left_image, (640, 480))\n",
    "    right_image = cv2.resize(right_image, (640, 480))\n",
    "\n",
    "    # Convert images to grayscale\n",
    "    gray_left = cv2.cvtColor(left_image, cv2.COLOR_BGR2GRAY)\n",
    "    gray_right = cv2.cvtColor(right_image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Detect Shi-Tomasi corners\n",
    "    corners_left = cv2.goodFeaturesToTrack(gray_left, maxCorners=maxCorners_my, qualityLevel=qualityLevel_my, minDistance=minDistance_my)\n",
    "    corners_right = cv2.goodFeaturesToTrack(gray_right, maxCorners=maxCorners_my, qualityLevel=qualityLevel_my, minDistance=minDistance_my)\n",
    "\n",
    "    # Convert Shi-Tomasi corners to keypoints for ORB descriptor computation\n",
    "    keypoints_left = [cv2.KeyPoint(x[0][0], x[0][1], 1) for x in corners_left] if corners_left is not None else []\n",
    "    keypoints_right = [cv2.KeyPoint(x[0][0], x[0][1], 1) for x in corners_right] if corners_right is not None else []\n",
    "\n",
    "    # Compute descriptors using ORB\n",
    "    keypoints_left, descriptors_left = orb.compute(gray_left, keypoints_left)\n",
    "    keypoints_right, descriptors_right = orb.compute(gray_right, keypoints_right)\n",
    "\n",
    "    # Match features using BFMatcher\n",
    "    bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "    matches = bf.match(descriptors_left, descriptors_right)\n",
    "    matches = sorted(matches, key=lambda x: x.distance)  # Sort matches by distance\n",
    "\n",
    "    # Draw matches\n",
    "    matched_image = cv2.drawMatches(left_image, keypoints_left, right_image, keypoints_right, matches[:50], None, flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    "\n",
    "    # Save the matched image if save_matches is True\n",
    "    if save_matches:\n",
    "        # Separate folders for left and right matches\n",
    "        output_left_path = os.path.join(output_folder, f\"Matched_Left_{idx:04d}.png\")\n",
    "        output_right_path = os.path.join(output_folder, f\"Matched_Right_{idx:04d}.png\")\n",
    "        output_combined_path = os.path.join(output_folder, f\"Matched_Combined_{idx:04d}.png\")\n",
    "\n",
    "        cv2.imwrite(output_combined_path, matched_image)\n",
    "        print(f\"Saved matched image: {output_combined_path}\")\n",
    "\n",
    "    # Display the matched image\n",
    "    cv2.imshow(\"Feature Matching\", matched_image)\n",
    "\n",
    "    # Wait for 'q' to quit or 100ms per frame\n",
    "    if cv2.waitKey(100) & 0xFF == ord('q'):\n",
    "        print(\"Exiting display...\")\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "print(\"Feature matching completed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display Matched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying feature matching...\n",
      "Exiting display...\n",
      "Feature matching display completed!\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Paths to the image folders\n",
    "left_folder = r\"C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Left3\"\n",
    "right_folder = r\"C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Right3\"\n",
    "\n",
    "# Shi-Tomasi parameters\n",
    "maxCorners_my = 300\n",
    "qualityLevel_my = 0.01\n",
    "minDistance_my = 15\n",
    "\n",
    "# ORB descriptor extractor\n",
    "orb = cv2.ORB_create()\n",
    "\n",
    "# Get sorted list of images\n",
    "left_images = sorted([os.path.join(left_folder, f) for f in os.listdir(left_folder) if f.endswith(('.png', '.jpg', '.jpeg'))])\n",
    "right_images = sorted([os.path.join(right_folder, f) for f in os.listdir(right_folder) if f.endswith(('.png', '.jpg', '.jpeg'))])\n",
    "\n",
    "# Ensure both folders have the same number of images\n",
    "if len(left_images) != len(right_images):\n",
    "    print(\"Error: The number of images in the left and right folders does not match.\")\n",
    "    exit()\n",
    "\n",
    "# Process images\n",
    "print(\"Displaying feature matching...\")\n",
    "for idx, (left_image_path, right_image_path) in enumerate(zip(left_images, right_images)):\n",
    "    # Read images\n",
    "    left_image = cv2.imread(left_image_path)\n",
    "    right_image = cv2.imread(right_image_path)\n",
    "\n",
    "    if left_image is None or right_image is None:\n",
    "        print(f\"Error reading images: {left_image_path}, {right_image_path}\")\n",
    "        continue\n",
    "\n",
    "    # Resize images to ensure the same dimensions\n",
    "    left_image = cv2.resize(left_image, (640, 480))\n",
    "    right_image = cv2.resize(right_image, (640, 480))\n",
    "\n",
    "    # Convert images to grayscale\n",
    "    gray_left = cv2.cvtColor(left_image, cv2.COLOR_BGR2GRAY)\n",
    "    gray_right = cv2.cvtColor(right_image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Detect Shi-Tomasi corners\n",
    "    corners_left = cv2.goodFeaturesToTrack(gray_left, maxCorners=maxCorners_my, qualityLevel=qualityLevel_my, minDistance=minDistance_my)\n",
    "    corners_right = cv2.goodFeaturesToTrack(gray_right, maxCorners=maxCorners_my, qualityLevel=qualityLevel_my, minDistance=minDistance_my)\n",
    "\n",
    "    # Convert Shi-Tomasi corners to keypoints for ORB descriptor computation\n",
    "    keypoints_left = [cv2.KeyPoint(x[0][0], x[0][1], 1) for x in corners_left] if corners_left is not None else []\n",
    "    keypoints_right = [cv2.KeyPoint(x[0][0], x[0][1], 1) for x in corners_right] if corners_right is not None else []\n",
    "\n",
    "    # Compute descriptors using ORB\n",
    "    keypoints_left, descriptors_left = orb.compute(gray_left, keypoints_left)\n",
    "    keypoints_right, descriptors_right = orb.compute(gray_right, keypoints_right)\n",
    "\n",
    "    # Match features using BFMatcher\n",
    "    bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "    matches = bf.match(descriptors_left, descriptors_right)\n",
    "    matches = sorted(matches, key=lambda x: x.distance)  # Sort matches by distance\n",
    "\n",
    "    # Draw matches\n",
    "    matched_image = cv2.drawMatches(left_image, keypoints_left, right_image, keypoints_right, matches[:50], None, flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    "\n",
    "    # Display the matched image\n",
    "    cv2.imshow(\"Feature Matching\", matched_image)\n",
    "\n",
    "    # Wait for 'q' to quit or 100ms per frame\n",
    "    if cv2.waitKey(100) & 0xFF == ord('q'):\n",
    "        print(\"Exiting display...\")\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "print(\"Feature matching display completed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply ORB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing images with ORB...\n",
      "Processed all images. No images saved.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "# Paths to the image folders\n",
    "left_folder = r\"C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Left1\"\n",
    "right_folder = r\"C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Right1\"\n",
    "\n",
    "# Output folders for processed images\n",
    "output_left_folder = r\"C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\ORB_Left1\"\n",
    "output_right_folder = r\"C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\ORB_Right1\"\n",
    "\n",
    "# User choice: Save images? (True/False)\n",
    "save_images = False  # Set to True if you want to save processed frames as images\n",
    "\n",
    "# Create output directories if saving is enabled\n",
    "if save_images:\n",
    "    os.makedirs(output_left_folder, exist_ok=True)\n",
    "    os.makedirs(output_right_folder, exist_ok=True)\n",
    "\n",
    "# ORB parameters\n",
    "orb = cv2.ORB_create(nfeatures=500)  # Adjust the number of features\n",
    "\n",
    "# Get sorted list of images\n",
    "left_images = sorted([os.path.join(left_folder, f) for f in os.listdir(left_folder) if f.endswith(('.png', '.jpg', '.jpeg'))])\n",
    "right_images = sorted([os.path.join(right_folder, f) for f in os.listdir(right_folder) if f.endswith(('.png', '.jpg', '.jpeg'))])\n",
    "\n",
    "# Ensure both folders have the same number of images\n",
    "if len(left_images) != len(right_images):\n",
    "    print(\"Error: The number of images in the left and right folders does not match.\")\n",
    "    exit()\n",
    "\n",
    "# Process images\n",
    "print(\"Processing images with ORB...\")\n",
    "for idx, (left_image_path, right_image_path) in enumerate(zip(left_images, right_images)):\n",
    "    # Read images\n",
    "    left_image = cv2.imread(left_image_path)\n",
    "    right_image = cv2.imread(right_image_path)\n",
    "\n",
    "    if left_image is None or right_image is None:\n",
    "        print(f\"Error reading images: {left_image_path}, {right_image_path}\")\n",
    "        continue\n",
    "\n",
    "    # Resize images to ensure the same dimensions\n",
    "    left_image = cv2.resize(left_image, (640, 480))\n",
    "    right_image = cv2.resize(right_image, (640, 480))\n",
    "\n",
    "    # Detect ORB keypoints and descriptors\n",
    "    keypoints_left, descriptors_left = orb.detectAndCompute(left_image, None)\n",
    "    keypoints_right, descriptors_right = orb.detectAndCompute(right_image, None)\n",
    "\n",
    "    # Draw keypoints on the images\n",
    "    left_image_with_keypoints = cv2.drawKeypoints(left_image, keypoints_left, None, color=(0, 255, 0), flags=0)\n",
    "    right_image_with_keypoints = cv2.drawKeypoints(right_image, keypoints_right, None, color=(0, 255, 0), flags=0)\n",
    "\n",
    "    # Save the processed frames as images if enabled\n",
    "    if save_images:\n",
    "        output_left_image_path = os.path.join(output_left_folder, f\"left_{idx:04d}.png\")\n",
    "        output_right_image_path = os.path.join(output_right_folder, f\"right_{idx:04d}.png\")\n",
    "        cv2.imwrite(output_left_image_path, left_image_with_keypoints)\n",
    "        cv2.imwrite(output_right_image_path, right_image_with_keypoints)\n",
    "        print(f\"Saved: {output_left_image_path}, {output_right_image_path}\")\n",
    "\n",
    "    # Combine left and right images side by side for display\n",
    "    combined_frame = cv2.hconcat([left_image_with_keypoints, right_image_with_keypoints])\n",
    "\n",
    "    # Display the combined frame\n",
    "    cv2.imshow(\"ORB Keypoints Detection\", combined_frame)\n",
    "\n",
    "    # Wait for 'q' to quit or 100ms per frame\n",
    "    if cv2.waitKey(100) & 0xFF == ord('q'):\n",
    "        print(\"Exiting display...\")\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "if save_images:\n",
    "    print(f\"Processed images saved in:\\nLeft folder: {output_left_folder}\\nRight folder: {output_right_folder}\")\n",
    "else:\n",
    "    print(\"Processed all images. No images saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply SIFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing images with SIFT...\n",
      "Processed all images. No images saved.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "# Paths to the image folders\n",
    "left_folder = r\"C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Left1\"\n",
    "right_folder = r\"C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Right1\"\n",
    "\n",
    "# Output folders for processed images\n",
    "output_left_folder = r\"C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\SIFT_Left1\"\n",
    "output_right_folder = r\"C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\SIFT_Right1\"\n",
    "\n",
    "# User choice: Save images? (True/False)\n",
    "save_images = False  # Set to True if you want to save processed frames as images\n",
    "\n",
    "# Create output directories if saving is enabled\n",
    "if save_images:\n",
    "    os.makedirs(output_left_folder, exist_ok=True)\n",
    "    os.makedirs(output_right_folder, exist_ok=True)\n",
    "\n",
    "# Initialize SIFT detector\n",
    "sift = cv2.SIFT_create()\n",
    "\n",
    "# Get sorted list of images\n",
    "left_images = sorted([os.path.join(left_folder, f) for f in os.listdir(left_folder) if f.endswith(('.png', '.jpg', '.jpeg'))])\n",
    "right_images = sorted([os.path.join(right_folder, f) for f in os.listdir(right_folder) if f.endswith(('.png', '.jpg', '.jpeg'))])\n",
    "\n",
    "# Ensure both folders have the same number of images\n",
    "if len(left_images) != len(right_images):\n",
    "    print(\"Error: The number of images in the left and right folders does not match.\")\n",
    "    exit()\n",
    "\n",
    "# Process images\n",
    "print(\"Processing images with SIFT...\")\n",
    "for idx, (left_image_path, right_image_path) in enumerate(zip(left_images, right_images)):\n",
    "    # Read images\n",
    "    left_image = cv2.imread(left_image_path)\n",
    "    right_image = cv2.imread(right_image_path)\n",
    "\n",
    "    if left_image is None or right_image is None:\n",
    "        print(f\"Error reading images: {left_image_path}, {right_image_path}\")\n",
    "        continue\n",
    "\n",
    "    # Resize images to ensure the same dimensions\n",
    "    left_image = cv2.resize(left_image, (640, 480))\n",
    "    right_image = cv2.resize(right_image, (640, 480))\n",
    "\n",
    "    # Convert images to grayscale\n",
    "    gray_left = cv2.cvtColor(left_image, cv2.COLOR_BGR2GRAY)\n",
    "    gray_right = cv2.cvtColor(right_image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Detect SIFT keypoints and descriptors\n",
    "    keypoints_left, descriptors_left = sift.detectAndCompute(gray_left, None)\n",
    "    keypoints_right, descriptors_right = sift.detectAndCompute(gray_right, None)\n",
    "\n",
    "    # Draw keypoints on the images\n",
    "    left_image_with_keypoints = cv2.drawKeypoints(left_image, keypoints_left, None, flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "    right_image_with_keypoints = cv2.drawKeypoints(right_image, keypoints_right, None, flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "\n",
    "    # Save the processed frames as images if enabled\n",
    "    if save_images:\n",
    "        output_left_image_path = os.path.join(output_left_folder, f\"left_{idx:04d}.png\")\n",
    "        output_right_image_path = os.path.join(output_right_folder, f\"right_{idx:04d}.png\")\n",
    "        cv2.imwrite(output_left_image_path, left_image_with_keypoints)\n",
    "        cv2.imwrite(output_right_image_path, right_image_with_keypoints)\n",
    "        print(f\"Saved: {output_left_image_path}, {output_right_image_path}\")\n",
    "\n",
    "    # Combine left and right images side by side for display\n",
    "    combined_frame = cv2.hconcat([left_image_with_keypoints, right_image_with_keypoints])\n",
    "\n",
    "    # Display the combined frame\n",
    "    cv2.imshow(\"SIFT Keypoints Detection\", combined_frame)\n",
    "\n",
    "    # Wait for 'q' to quit or 100ms per frame\n",
    "    if cv2.waitKey(100) & 0xFF == ord('q'):\n",
    "        print(\"Exiting display...\")\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "if save_images:\n",
    "    print(f\"Processed images saved in:\\nLeft folder: {output_left_folder}\\nRight folder: {output_right_folder}\")\n",
    "else:\n",
    "    print(\"Processed all images. No images saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Not Worked --> Apply SURF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'cv2' has no attribute 'xfeatures2d'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 21\u001b[0m\n\u001b[0;32m     18\u001b[0m     os\u001b[38;5;241m.\u001b[39mmakedirs(output_right_folder, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Initialize SURF detector\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m surf \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mxfeatures2d\u001b[38;5;241m.\u001b[39mSURF_create(hessianThreshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m400\u001b[39m)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Get sorted list of images\u001b[39;00m\n\u001b[0;32m     24\u001b[0m left_images \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m([os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(left_folder, f) \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(left_folder) \u001b[38;5;28;01mif\u001b[39;00m f\u001b[38;5;241m.\u001b[39mendswith((\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.png\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.jpeg\u001b[39m\u001b[38;5;124m'\u001b[39m))])\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'cv2' has no attribute 'xfeatures2d'"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "# Paths to the image folders\n",
    "left_folder = r\"C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Left1\"\n",
    "right_folder = r\"C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Right1\"\n",
    "\n",
    "# Output folders for processed images\n",
    "output_left_folder = r\"C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\SURF_Left1\"\n",
    "output_right_folder = r\"C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\SURF_Right1\"\n",
    "\n",
    "# User choice: Save images? (True/False)\n",
    "save_images = False  # Set to True if you want to save processed frames as images\n",
    "\n",
    "# Create output directories if saving is enabled\n",
    "if save_images:\n",
    "    os.makedirs(output_left_folder, exist_ok=True)\n",
    "    os.makedirs(output_right_folder, exist_ok=True)\n",
    "\n",
    "# Initialize SURF detector\n",
    "surf = cv2.xfeatures2d.SURF_create(hessianThreshold=400)\n",
    "\n",
    "# Get sorted list of images\n",
    "left_images = sorted([os.path.join(left_folder, f) for f in os.listdir(left_folder) if f.endswith(('.png', '.jpg', '.jpeg'))])\n",
    "right_images = sorted([os.path.join(right_folder, f) for f in os.listdir(right_folder) if f.endswith(('.png', '.jpg', '.jpeg'))])\n",
    "\n",
    "# Ensure both folders have the same number of images\n",
    "if len(left_images) != len(right_images):\n",
    "    print(\"Error: The number of images in the left and right folders does not match.\")\n",
    "    exit()\n",
    "\n",
    "# Process images\n",
    "print(\"Processing images with SURF...\")\n",
    "for idx, (left_image_path, right_image_path) in enumerate(zip(left_images, right_images)):\n",
    "    # Read images\n",
    "    left_image = cv2.imread(left_image_path)\n",
    "    right_image = cv2.imread(right_image_path)\n",
    "\n",
    "    if left_image is None or right_image is None:\n",
    "        print(f\"Error reading images: {left_image_path}, {right_image_path}\")\n",
    "        continue\n",
    "\n",
    "    # Resize images to ensure the same dimensions\n",
    "    left_image = cv2.resize(left_image, (640, 480))\n",
    "    right_image = cv2.resize(right_image, (640, 480))\n",
    "\n",
    "    # Convert images to grayscale\n",
    "    gray_left = cv2.cvtColor(left_image, cv2.COLOR_BGR2GRAY)\n",
    "    gray_right = cv2.cvtColor(right_image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Detect SURF keypoints and descriptors\n",
    "    keypoints_left, descriptors_left = surf.detectAndCompute(gray_left, None)\n",
    "    keypoints_right, descriptors_right = surf.detectAndCompute(gray_right, None)\n",
    "\n",
    "    # Draw keypoints on the images\n",
    "    left_image_with_keypoints = cv2.drawKeypoints(left_image, keypoints_left, None, color=(0, 255, 0), flags=0)\n",
    "    right_image_with_keypoints = cv2.drawKeypoints(right_image, keypoints_right, None, color=(0, 255, 0), flags=0)\n",
    "\n",
    "    # Save the processed frames as images if enabled\n",
    "    if save_images:\n",
    "        output_left_image_path = os.path.join(output_left_folder, f\"left_{idx:04d}.png\")\n",
    "        output_right_image_path = os.path.join(output_right_folder, f\"right_{idx:04d}.png\")\n",
    "        cv2.imwrite(output_left_image_path, left_image_with_keypoints)\n",
    "        cv2.imwrite(output_right_image_path, right_image_with_keypoints)\n",
    "        print(f\"Saved: {output_left_image_path}, {output_right_image_path}\")\n",
    "\n",
    "    # Combine left and right images side by side for display\n",
    "    combined_frame = cv2.hconcat([left_image_with_keypoints, right_image_with_keypoints])\n",
    "\n",
    "    # Display the combined frame\n",
    "    cv2.imshow(\"SURF Keypoints Detection\", combined_frame)\n",
    "\n",
    "    # Wait for 'q' to quit or 100ms per frame\n",
    "    if cv2.waitKey(100) & 0xFF == ord('q'):\n",
    "        print(\"Exiting display...\")\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "if save_images:\n",
    "    print(f\"Processed images saved in:\\nLeft folder: {output_left_folder}\\nRight folder: {output_right_folder}\")\n",
    "else:\n",
    "    print(\"Processed all images. No images saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Not Worked --> Apply BRIEF as Detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'cv2' has no attribute 'xfeatures2d'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 23\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Initialize FAST detector and BRIEF descriptor\u001b[39;00m\n\u001b[0;32m     22\u001b[0m fast \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mFastFeatureDetector_create()  \u001b[38;5;66;03m# FAST keypoint detector\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m brief \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mxfeatures2d\u001b[38;5;241m.\u001b[39mBriefDescriptorExtractor_create()  \u001b[38;5;66;03m# BRIEF descriptor extractor\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Get sorted list of images\u001b[39;00m\n\u001b[0;32m     26\u001b[0m left_images \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m([os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(left_folder, f) \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(left_folder) \u001b[38;5;28;01mif\u001b[39;00m f\u001b[38;5;241m.\u001b[39mendswith((\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.png\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.jpeg\u001b[39m\u001b[38;5;124m'\u001b[39m))])\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'cv2' has no attribute 'xfeatures2d'"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Paths to the image folders\n",
    "left_folder = r\"C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Left1\"\n",
    "right_folder = r\"C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\Right1\"\n",
    "\n",
    "# Output folders for processed images\n",
    "output_left_folder = r\"C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\BRIEF_Left1\"\n",
    "output_right_folder = r\"C:\\Users\\webin\\OneDrive\\Desktop\\For Senior Video\\Mock_two_cam_26_Nov\\BRIEF_Right1\"\n",
    "\n",
    "# User choice: Save images? (True/False)\n",
    "save_images = False  # Set to True if you want to save processed frames as images\n",
    "\n",
    "# Create output directories if saving is enabled\n",
    "if save_images:\n",
    "    os.makedirs(output_left_folder, exist_ok=True)\n",
    "    os.makedirs(output_right_folder, exist_ok=True)\n",
    "\n",
    "# Initialize FAST detector and BRIEF descriptor\n",
    "fast = cv2.FastFeatureDetector_create()  # FAST keypoint detector\n",
    "brief = cv2.xfeatures2d.BriefDescriptorExtractor_create()  # BRIEF descriptor extractor\n",
    "\n",
    "# Get sorted list of images\n",
    "left_images = sorted([os.path.join(left_folder, f) for f in os.listdir(left_folder) if f.endswith(('.png', '.jpg', '.jpeg'))])\n",
    "right_images = sorted([os.path.join(right_folder, f) for f in os.listdir(right_folder) if f.endswith(('.png', '.jpg', '.jpeg'))])\n",
    "\n",
    "# Ensure both folders have the same number of images\n",
    "if len(left_images) != len(right_images):\n",
    "    print(\"Error: The number of images in the left and right folders does not match.\")\n",
    "    exit()\n",
    "\n",
    "# Process images\n",
    "print(\"Processing images with BRIEF...\")\n",
    "for idx, (left_image_path, right_image_path) in enumerate(zip(left_images, right_images)):\n",
    "    # Read images\n",
    "    left_image = cv2.imread(left_image_path)\n",
    "    right_image = cv2.imread(right_image_path)\n",
    "\n",
    "    if left_image is None or right_image is None:\n",
    "        print(f\"Error reading images: {left_image_path}, {right_image_path}\")\n",
    "        continue\n",
    "\n",
    "    # Resize images to ensure the same dimensions\n",
    "    left_image = cv2.resize(left_image, (640, 480))\n",
    "    right_image = cv2.resize(right_image, (640, 480))\n",
    "\n",
    "    # Convert images to grayscale\n",
    "    gray_left = cv2.cvtColor(left_image, cv2.COLOR_BGR2GRAY)\n",
    "    gray_right = cv2.cvtColor(right_image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Detect FAST keypoints\n",
    "    keypoints_left = fast.detect(gray_left, None)\n",
    "    keypoints_right = fast.detect(gray_right, None)\n",
    "\n",
    "    # Compute BRIEF descriptors\n",
    "    keypoints_left, descriptors_left = brief.compute(gray_left, keypoints_left)\n",
    "    keypoints_right, descriptors_right = brief.compute(gray_right, keypoints_right)\n",
    "\n",
    "    # Draw keypoints on the images\n",
    "    left_image_with_keypoints = cv2.drawKeypoints(left_image, keypoints_left, None, color=(0, 255, 0), flags=0)\n",
    "    right_image_with_keypoints = cv2.drawKeypoints(right_image, keypoints_right, None, color=(0, 255, 0), flags=0)\n",
    "\n",
    "    # Save the processed frames as images if enabled\n",
    "    if save_images:\n",
    "        output_left_image_path = os.path.join(output_left_folder, f\"left_{idx:04d}.png\")\n",
    "        output_right_image_path = os.path.join(output_right_folder, f\"right_{idx:04d}.png\")\n",
    "        cv2.imwrite(output_left_image_path, left_image_with_keypoints)\n",
    "        cv2.imwrite(output_right_image_path, right_image_with_keypoints)\n",
    "        print(f\"Saved: {output_left_image_path}, {output_right_image_path}\")\n",
    "\n",
    "    # Combine left and right images side by side for display\n",
    "    combined_frame = cv2.hconcat([left_image_with_keypoints, right_image_with_keypoints])\n",
    "\n",
    "    # Display the combined frame\n",
    "    cv2.imshow(\"BRIEF Keypoints Detection\", combined_frame)\n",
    "\n",
    "    # Wait for 'q' to quit or 100ms per frame\n",
    "    if cv2.waitKey(100) & 0xFF == ord('q'):\n",
    "        print(\"Exiting display...\")\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "if save_images:\n",
    "    print(f\"Processed images saved in:\\nLeft folder: {output_left_folder}\\nRight folder: {output_right_folder}\")\n",
    "else:\n",
    "    print(\"Processed all images. No images saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
